{
 "cells": [
  {
   "cell_type": "raw",
   "id": "eb3a982b",
   "metadata": {},
   "source": [
    "TARGET ->>\n",
    "def train(self, text, vocab_size, verbose=False)\n",
    "def encode(self, text)\n",
    "def decode(self, ids)\n",
    "\n",
    "Dataset = taylorswift.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd6e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids,counts = None):\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids[:],ids[1:]):   # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair,0)+1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f1cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ids = list of integer\n",
    "# pair = pair of consecutive index that we are going to replace\n",
    "# idx = replace it to index token idx\n",
    "def merge(ids,pair,idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if ids[i]== pair[0] and i < len(ids)-1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i +=2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i +=1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1962ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace control characters =  non-printing character like \\n\n",
    "# we don't want to print control characters\n",
    "def replace_control_characters(s:str)-> str:\n",
    "    chars = []\n",
    "    for ch in s:\n",
    "        if unicodedata.category(ch)[0] != \"C\":\n",
    "            chars.append(ch) # this character is ok\n",
    "        else:\n",
    "            chars.append(f\"\\\\u{ord(ch):04x}\")\n",
    "    return \"\".join(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19bceac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nicer way to decode / print the final text by not printing control characters\n",
    "def render_token(t: bytes)-> str:\n",
    "    s = t.decode('utf-8',errors='replace')\n",
    "    s = replace_control_characters(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db46f8be",
   "metadata": {},
   "source": [
    "BASE TOKENIZER CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f29dba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.merges = {} # (int,int) -> int\n",
    "        self.pattern = \"\" # string\n",
    "        self.special_tokens = {} #str -> int ,e.g. {'<|endoftext|>': 100257}\n",
    "        self.vocab = self._build_vocab()  # int -> bytes\n",
    "        \n",
    "    def train(self,text,vocab_size, verbose = False):\n",
    "        # Tokenizer can train a vocabulary of size vocab_size from text\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    def encode(self,text):\n",
    "        # Tokenizer can encode a string into a list of integers\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        # Tokenizer can decode a list of integers into a string\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _build_vocab(self):\n",
    "        # vocab is simply and deterministically derived from merges\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0,p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "            \n",
    "        for special,idx in self.special_tokens.items():\n",
    "            vocab[idx]= special.encode(\"utf-8\")\n",
    "            \n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1944eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer(Tokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def train(self,text,vocab_size,verbose = False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256 \n",
    "        \n",
    "        #input text pre-processing \n",
    "        text_bytes = text.encode(\"utf-8\") # raw bytes \n",
    "        ids = list(text_bytes) # list of integers in range 0..255\n",
    "        \n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {} # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "             # count up the number of times every consecutive pair appears\n",
    "            stats = get_stats(ids)\n",
    "            \n",
    "             # find the pair with the highest count\n",
    "            pair = max(stats,key = stats.get)\n",
    "            \n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 +i\n",
    "            \n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = merge(ids,pair,idx)\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            \n",
    "            # print - easier for debugging \n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges} : {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurences\")\n",
    "        \n",
    "        # save class variables for future use\n",
    "        self.merges = merges  # it will be used in encode()\n",
    "        self.vocab = vocab   # it will be used in decode()\n",
    "        \n",
    "    def decode(self,ids):\n",
    "        # given ids (list of integers), return Python string\n",
    "        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = text_bytes.decode(\"utf-8\",errors = \"replace\")\n",
    "        return text\n",
    "        \n",
    "    def encode(self,text):\n",
    "        # given a string text, return the token ids\n",
    "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "        ids = list(text_bytes) # list of integers in range 0..255\n",
    "        while len(ids) >=2:\n",
    "            # find the pair with the lowest merge index -> it must be the latest merge \n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats,key = lambda p: self.merges.get(p,float(\"inf\")))\n",
    "            #if there are no more merges available, the key will\n",
    "            # result in an inf for every single pair, and the min will be\n",
    "            # just the first pair in the list,\n",
    "            \n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged anymore\n",
    "            \n",
    "            # otherwise let's merge the best pair (lowest merge index)\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids,pair,idx)\n",
    "        \n",
    "        return ids\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d085b904",
   "metadata": {},
   "source": [
    "Small example:\n",
    "text = \"Hello Aryan\"\n",
    "tb = text.encode(\"utf-8\")\n",
    "tb = b'Hello Aryan'\n",
    "list(tb) = [72, 101, 108, 108, 111, 32, 65, 114, 121, 97, 110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a1eaeea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('taylorswift.txt','r',encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7336df59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copy paste of the Wi'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "46e78d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BasicTokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd376b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copy paste of the Wikipedia article on Taylor Swift, as of Feb 16, 2024.\\n---\\n\\nMain menu\\n\\nWikipediaTh'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "47c910a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3fb5fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/256 : (101, 32) -> 256 (b'e ') had 2981 occurences\n",
      "merge 2/256 : (44, 32) -> 257 (b', ') had 2961 occurences\n",
      "merge 3/256 : (100, 32) -> 258 (b'd ') had 2617 occurences\n",
      "merge 4/256 : (46, 32) -> 259 (b'. ') had 2560 occurences\n",
      "merge 5/256 : (114, 32) -> 260 (b'r ') had 2428 occurences\n",
      "merge 6/256 : (50, 48) -> 261 (b'20') had 2365 occurences\n",
      "merge 7/256 : (115, 32) -> 262 (b's ') had 2053 occurences\n",
      "merge 8/256 : (105, 110) -> 263 (b'in') had 2006 occurences\n",
      "merge 9/256 : (111, 110) -> 264 (b'on') had 1815 occurences\n",
      "merge 10/256 : (114, 105) -> 265 (b'ri') had 1805 occurences\n",
      "merge 11/256 : (116, 32) -> 266 (b't ') had 1802 occurences\n",
      "merge 12/256 : (116, 104) -> 267 (b'th') had 1737 occurences\n",
      "merge 13/256 : (101, 258) -> 268 (b'ed ') had 1736 occurences\n",
      "merge 14/256 : (257, 261) -> 269 (b', 20') had 1705 occurences\n",
      "merge 15/256 : (97, 110) -> 270 (b'an') had 1487 occurences\n",
      "merge 16/256 : (97, 114) -> 271 (b'ar') had 1360 occurences\n",
      "merge 17/256 : (101, 260) -> 272 (b'er ') had 1356 occurences\n",
      "merge 18/256 : (121, 32) -> 273 (b'y ') had 1248 occurences\n",
      "merge 19/256 : (97, 108) -> 274 (b'al') had 1164 occurences\n",
      "merge 20/256 : (267, 256) -> 275 (b'the ') had 1142 occurences\n",
      "merge 21/256 : (118, 268) -> 276 (b'ved ') had 1104 occurences\n",
      "merge 22/256 : (119, 105) -> 277 (b'wi') had 1049 occurences\n",
      "merge 23/256 : (101, 114) -> 278 (b'er') had 897 occurences\n",
      "merge 24/256 : (264, 32) -> 279 (b'on ') had 880 occurences\n",
      "merge 25/256 : (277, 102) -> 280 (b'wif') had 871 occurences\n",
      "merge 26/256 : (82, 101) -> 281 (b'Re') had 870 occurences\n",
      "merge 27/256 : (83, 280) -> 282 (b'Swif') had 867 occurences\n",
      "merge 28/256 : (111, 260) -> 283 (b'or ') had 859 occurences\n",
      "merge 29/256 : (99, 104) -> 284 (b'ch') had 816 occurences\n",
      "merge 30/256 : (269, 49) -> 285 (b', 201') had 811 occurences\n",
      "merge 31/256 : (111, 109) -> 286 (b'om') had 789 occurences\n",
      "merge 32/256 : (98, 272) -> 287 (b'ber ') had 752 occurences\n",
      "merge 33/256 : (32, 275) -> 288 (b' the ') had 748 occurences\n",
      "merge 34/256 : (97, 121) -> 289 (b'ay') had 744 occurences\n",
      "merge 35/256 : (101, 110) -> 290 (b'en') had 740 occurences\n",
      "merge 36/256 : (111, 114) -> 291 (b'or') had 737 occurences\n",
      "merge 37/256 : (274, 32) -> 292 (b'al ') had 705 occurences\n",
      "merge 38/256 : (101, 109) -> 293 (b'em') had 703 occurences\n",
      "merge 39/256 : (46, 10) -> 294 (b'.\\n') had 685 occurences\n",
      "merge 40/256 : (265, 101) -> 295 (b'rie') had 685 occurences\n",
      "merge 41/256 : (263, 103) -> 296 (b'ing') had 684 occurences\n",
      "merge 42/256 : (269, 50) -> 297 (b', 202') had 673 occurences\n",
      "merge 43/256 : (116, 105) -> 298 (b'ti') had 666 occurences\n",
      "merge 44/256 : (289, 108) -> 299 (b'ayl') had 654 occurences\n",
      "merge 45/256 : (34, 259) -> 300 (b'\". ') had 651 occurences\n",
      "merge 46/256 : (108, 108) -> 301 (b'll') had 649 occurences\n",
      "merge 47/256 : (84, 299) -> 302 (b'Tayl') had 647 occurences\n",
      "merge 48/256 : (116, 295) -> 303 (b'trie') had 644 occurences\n",
      "merge 49/256 : (294, 32) -> 304 (b'.\\n ') had 643 occurences\n",
      "merge 50/256 : (116, 111) -> 305 (b'to') had 642 occurences\n",
      "merge 51/256 : (259, 281) -> 306 (b'. Re') had 640 occurences\n",
      "merge 52/256 : (306, 303) -> 307 (b'. Retrie') had 639 occurences\n",
      "merge 53/256 : (307, 276) -> 308 (b'. Retrieved ') had 639 occurences\n",
      "merge 54/256 : (302, 283) -> 309 (b'Taylor ') had 611 occurences\n",
      "merge 55/256 : (101, 115) -> 310 (b'es') had 606 occurences\n",
      "merge 56/256 : (309, 282) -> 311 (b'Taylor Swif') had 598 occurences\n",
      "merge 57/256 : (117, 115) -> 312 (b'us') had 561 occurences\n",
      "merge 58/256 : (114, 286) -> 313 (b'rom') had 532 occurences\n",
      "merge 59/256 : (293, 287) -> 314 (b'ember ') had 528 occurences\n",
      "merge 60/256 : (41, 259) -> 315 (b'). ') had 524 occurences\n",
      "merge 61/256 : (65, 114) -> 316 (b'Ar') had 509 occurences\n",
      "merge 62/256 : (102, 313) -> 317 (b'from') had 503 occurences\n",
      "merge 63/256 : (315, 34) -> 318 (b'). \"') had 499 occurences\n",
      "merge 64/256 : (270, 258) -> 319 (b'and ') had 498 occurences\n",
      "merge 65/256 : (114, 101) -> 320 (b're') had 495 occurences\n",
      "merge 66/256 : (111, 117) -> 321 (b'ou') had 487 occurences\n",
      "merge 67/256 : (111, 265) -> 322 (b'ori') had 469 occurences\n",
      "merge 68/256 : (111, 102) -> 323 (b'of') had 466 occurences\n",
      "merge 69/256 : (103, 263) -> 324 (b'gin') had 465 occurences\n",
      "merge 70/256 : (296, 32) -> 325 (b'ing ') had 464 occurences\n",
      "merge 71/256 : (284, 105) -> 326 (b'chi') had 458 occurences\n",
      "merge 72/256 : (93, 32) -> 327 (b'] ') had 458 occurences\n",
      "merge 73/256 : (324, 292) -> 328 (b'ginal ') had 453 occurences\n",
      "merge 74/256 : (317, 288) -> 329 (b'from the ') had 447 occurences\n",
      "merge 75/256 : (322, 328) -> 330 (b'original ') had 446 occurences\n",
      "merge 76/256 : (104, 256) -> 331 (b'he ') had 440 occurences\n",
      "merge 77/256 : (316, 326) -> 332 (b'Archi') had 440 occurences\n",
      "merge 78/256 : (332, 276) -> 333 (b'Archived ') had 440 occurences\n",
      "merge 79/256 : (329, 330) -> 334 (b'from the original ') had 440 occurences\n",
      "merge 80/256 : (333, 334) -> 335 (b'Archived from the original ') had 439 occurences\n",
      "merge 81/256 : (335, 279) -> 336 (b'Archived from the original on ') had 438 occurences\n",
      "merge 82/256 : (259, 336) -> 337 (b'. Archived from the original on ') had 433 occurences\n",
      "merge 83/256 : (97, 32) -> 338 (b'a ') had 420 occurences\n",
      "merge 84/256 : (115, 116) -> 339 (b'st') had 409 occurences\n",
      "merge 85/256 : (105, 99) -> 340 (b'ic') had 406 occurences\n",
      "merge 86/256 : (46, 91) -> 341 (b'.[') had 381 occurences\n",
      "merge 87/256 : (101, 99) -> 342 (b'ec') had 374 occurences\n",
      "merge 88/256 : (105, 301) -> 343 (b'ill') had 367 occurences\n",
      "merge 89/256 : (39, 262) -> 344 (b\"'s \") had 367 occurences\n",
      "merge 90/256 : (311, 266) -> 345 (b'Taylor Swift ') had 352 occurences\n",
      "merge 91/256 : (111, 118) -> 346 (b'ov') had 343 occurences\n",
      "merge 92/256 : (97, 116) -> 347 (b'at') had 334 occurences\n",
      "merge 93/256 : (97, 262) -> 348 (b'as ') had 315 occurences\n",
      "merge 94/256 : (101, 262) -> 349 (b'es ') had 309 occurences\n",
      "merge 95/256 : (74, 117) -> 350 (b'Ju') had 307 occurences\n",
      "merge 96/256 : (323, 32) -> 351 (b'of ') had 306 occurences\n",
      "merge 97/256 : (305, 32) -> 352 (b'to ') had 284 occurences\n",
      "merge 98/256 : (117, 109) -> 353 (b'um') had 281 occurences\n",
      "merge 99/256 : (84, 331) -> 354 (b'The ') had 277 occurences\n",
      "merge 100/256 : (271, 100) -> 355 (b'ard') had 277 occurences\n",
      "merge 101/256 : (263, 32) -> 356 (b'in ') had 276 occurences\n",
      "merge 102/256 : (270, 32) -> 357 (b'an ') had 276 occurences\n",
      "merge 103/256 : (101, 108) -> 358 (b'el') had 275 occurences\n",
      "merge 104/256 : (297, 51) -> 359 (b', 2023') had 271 occurences\n",
      "merge 105/256 : (271, 273) -> 360 (b'ary ') had 259 occurences\n",
      "merge 106/256 : (267, 32) -> 361 (b'th ') had 258 occurences\n",
      "merge 107/256 : (97, 109) -> 362 (b'am') had 257 occurences\n",
      "merge 108/256 : (108, 273) -> 363 (b'ly ') had 250 occurences\n",
      "merge 109/256 : (111, 112) -> 364 (b'op') had 249 occurences\n",
      "merge 110/256 : (311, 116) -> 365 (b'Taylor Swift') had 246 occurences\n",
      "merge 111/256 : (116, 114) -> 366 (b'tr') had 243 occurences\n",
      "merge 112/256 : (105, 115) -> 367 (b'is') had 234 occurences\n",
      "merge 113/256 : (104, 272) -> 368 (b'her ') had 232 occurences\n",
      "merge 114/256 : (111, 32) -> 369 (b'o ') had 225 occurences\n",
      "merge 115/256 : (117, 360) -> 370 (b'uary ') had 225 occurences\n",
      "merge 116/256 : (78, 346) -> 371 (b'Nov') had 222 occurences\n",
      "merge 117/256 : (312, 340) -> 372 (b'usic') had 221 occurences\n",
      "merge 118/256 : (371, 314) -> 373 (b'November ') had 221 occurences\n",
      "merge 119/256 : (101, 119) -> 374 (b'ew') had 219 occurences\n",
      "merge 120/256 : (97, 266) -> 375 (b'at ') had 219 occurences\n",
      "merge 121/256 : (108, 32) -> 376 (b'l ') had 218 occurences\n",
      "merge 122/256 : (58, 32) -> 377 (b': ') had 213 occurences\n",
      "merge 123/256 : (98, 111) -> 378 (b'bo') had 210 occurences\n",
      "merge 124/256 : (282, 266) -> 379 (b'Swift ') had 208 occurences\n",
      "merge 125/256 : (68, 342) -> 380 (b'Dec') had 207 occurences\n",
      "merge 126/256 : (105, 116) -> 381 (b'it') had 206 occurences\n",
      "merge 127/256 : (105, 103) -> 382 (b'ig') had 205 occurences\n",
      "merge 128/256 : (66, 343) -> 383 (b'Bill') had 205 occurences\n",
      "merge 129/256 : (49, 48) -> 384 (b'10') had 204 occurences\n",
      "merge 130/256 : (97, 115) -> 385 (b'as') had 203 occurences\n",
      "merge 131/256 : (264, 103) -> 386 (b'ong') had 202 occurences\n",
      "merge 132/256 : (79, 99) -> 387 (b'Oc') had 200 occurences\n",
      "merge 133/256 : (97, 298) -> 388 (b'ati') had 199 occurences\n",
      "merge 134/256 : (83, 116) -> 389 (b'St') had 198 occurences\n",
      "merge 135/256 : (387, 305) -> 390 (b'Octo') had 198 occurences\n",
      "merge 136/256 : (390, 287) -> 391 (b'October ') had 198 occurences\n",
      "merge 137/256 : (97, 99) -> 392 (b'ac') had 197 occurences\n",
      "merge 138/256 : (111, 119) -> 393 (b'ow') had 196 occurences\n",
      "merge 139/256 : (380, 314) -> 394 (b'December ') had 194 occurences\n",
      "merge 140/256 : (383, 378) -> 395 (b'Billbo') had 191 occurences\n",
      "merge 141/256 : (97, 100) -> 396 (b'ad') had 190 occurences\n",
      "merge 142/256 : (108, 101) -> 397 (b'le') had 190 occurences\n",
      "merge 143/256 : (117, 114) -> 398 (b'ur') had 188 occurences\n",
      "merge 144/256 : (102, 283) -> 399 (b'for ') had 188 occurences\n",
      "merge 145/256 : (32, 40) -> 400 (b' (') had 187 occurences\n",
      "merge 146/256 : (297, 50) -> 401 (b', 2022') had 187 occurences\n",
      "merge 147/256 : (117, 103) -> 402 (b'ug') had 185 occurences\n",
      "merge 148/256 : (284, 32) -> 403 (b'ch ') had 184 occurences\n",
      "merge 149/256 : (115, 266) -> 404 (b'st ') had 181 occurences\n",
      "merge 150/256 : (321, 110) -> 405 (b'oun') had 176 occurences\n",
      "merge 151/256 : (98, 353) -> 406 (b'bum') had 172 occurences\n",
      "merge 152/256 : (111, 108) -> 407 (b'ol') had 171 occurences\n",
      "merge 153/256 : (312, 266) -> 408 (b'ust ') had 171 occurences\n",
      "merge 154/256 : (101, 98) -> 409 (b'eb') had 170 occurences\n",
      "merge 155/256 : (77, 97) -> 410 (b'Ma') had 170 occurences\n",
      "merge 156/256 : (350, 363) -> 411 (b'July ') had 170 occurences\n",
      "merge 157/256 : (318, 345) -> 412 (b'). \"Taylor Swift ') had 169 occurences\n",
      "merge 158/256 : (107, 32) -> 413 (b'k ') had 165 occurences\n",
      "merge 159/256 : (278, 115) -> 414 (b'ers') had 164 occurences\n",
      "merge 160/256 : (93, 91) -> 415 (b'][') had 164 occurences\n",
      "merge 161/256 : (65, 402) -> 416 (b'Aug') had 164 occurences\n",
      "merge 162/256 : (416, 408) -> 417 (b'August ') had 163 occurences\n",
      "merge 163/256 : (105, 100) -> 418 (b'id') had 161 occurences\n",
      "merge 164/256 : (297, 49) -> 419 (b', 2021') had 160 occurences\n",
      "merge 165/256 : (109, 101) -> 420 (b'me') had 159 occurences\n",
      "merge 166/256 : (101, 112) -> 421 (b'ep') had 156 occurences\n",
      "merge 167/256 : (261, 49) -> 422 (b'201') had 149 occurences\n",
      "merge 168/256 : (50, 51) -> 423 (b'23') had 145 occurences\n",
      "merge 169/256 : (285, 50) -> 424 (b', 2012') had 144 occurences\n",
      "merge 170/256 : (101, 271) -> 425 (b'ear') had 140 occurences\n",
      "merge 171/256 : (269, 261) -> 426 (b', 2020') had 140 occurences\n",
      "merge 172/256 : (73, 110) -> 427 (b'In') had 139 occurences\n",
      "merge 173/256 : (102, 105) -> 428 (b'fi') had 139 occurences\n",
      "merge 174/256 : (110, 256) -> 429 (b'ne ') had 139 occurences\n",
      "merge 175/256 : (395, 355) -> 430 (b'Billboard') had 136 occurences\n",
      "merge 176/256 : (265, 116) -> 431 (b'rit') had 134 occurences\n",
      "merge 177/256 : (104, 105) -> 432 (b'hi') had 133 occurences\n",
      "merge 178/256 : (372, 32) -> 433 (b'usic ') had 133 occurences\n",
      "merge 179/256 : (304, 34) -> 434 (b'.\\n \"') had 133 occurences\n",
      "merge 180/256 : (78, 374) -> 435 (b'New') had 131 occurences\n",
      "merge 181/256 : (100, 105) -> 436 (b'di') had 130 occurences\n",
      "merge 182/256 : (65, 112) -> 437 (b'Ap') had 130 occurences\n",
      "merge 183/256 : (285, 57) -> 438 (b', 2019') had 129 occurences\n",
      "merge 184/256 : (114, 111) -> 439 (b'ro') had 128 occurences\n",
      "merge 185/256 : (39, 32) -> 440 (b\"' \") had 128 occurences\n",
      "merge 186/256 : (115, 257) -> 441 (b's, ') had 127 occurences\n",
      "merge 187/256 : (350, 429) -> 442 (b'June ') had 127 occurences\n",
      "merge 188/256 : (323, 288) -> 443 (b'of the ') had 126 occurences\n",
      "merge 189/256 : (99, 291) -> 444 (b'cor') had 126 occurences\n",
      "merge 190/256 : (50, 49) -> 445 (b'21') had 126 occurences\n",
      "merge 191/256 : (49, 57) -> 446 (b'19') had 124 occurences\n",
      "merge 192/256 : (105, 109) -> 447 (b'im') had 123 occurences\n",
      "merge 193/256 : (290, 32) -> 448 (b'en ') had 123 occurences\n",
      "merge 194/256 : (409, 114) -> 449 (b'ebr') had 122 occurences\n",
      "merge 195/256 : (290, 116) -> 450 (b'ent') had 121 occurences\n",
      "merge 196/256 : (111, 301) -> 451 (b'oll') had 121 occurences\n",
      "merge 197/256 : (77, 271) -> 452 (b'Mar') had 120 occurences\n",
      "merge 198/256 : (265, 99) -> 453 (b'ric') had 120 occurences\n",
      "merge 199/256 : (277, 361) -> 454 (b'with ') had 120 occurences\n",
      "merge 200/256 : (44, 91) -> 455 (b',[') had 118 occurences\n",
      "merge 201/256 : (70, 449) -> 456 (b'Febr') had 118 occurences\n",
      "merge 202/256 : (456, 370) -> 457 (b'February ') had 118 occurences\n",
      "merge 203/256 : (365, 344) -> 458 (b\"Taylor Swift's \") had 118 occurences\n",
      "merge 204/256 : (300, 430) -> 459 (b'\". Billboard') had 118 occurences\n",
      "merge 205/256 : (101, 97) -> 460 (b'ea') had 116 occurences\n",
      "merge 206/256 : (285, 54) -> 461 (b', 2016') had 116 occurences\n",
      "merge 207/256 : (421, 116) -> 462 (b'ept') had 115 occurences\n",
      "merge 208/256 : (410, 273) -> 463 (b'May ') had 115 occurences\n",
      "merge 209/256 : (285, 53) -> 464 (b', 2015') had 115 occurences\n",
      "merge 210/256 : (437, 265) -> 465 (b'Apri') had 115 occurences\n",
      "merge 211/256 : (465, 376) -> 466 (b'April ') had 115 occurences\n",
      "merge 212/256 : (108, 256) -> 467 (b'le ') had 113 occurences\n",
      "merge 213/256 : (65, 119) -> 468 (b'Aw') had 112 occurences\n",
      "merge 214/256 : (388, 264) -> 469 (b'ation') had 112 occurences\n",
      "merge 215/256 : (83, 462) -> 470 (b'Sept') had 112 occurences\n",
      "merge 216/256 : (470, 314) -> 471 (b'September ') had 112 occurences\n",
      "merge 217/256 : (114, 97) -> 472 (b'ra') had 111 occurences\n",
      "merge 218/256 : (274, 406) -> 473 (b'album') had 111 occurences\n",
      "merge 219/256 : (67, 104) -> 474 (b'Ch') had 110 occurences\n",
      "merge 220/256 : (118, 256) -> 475 (b've ') had 109 occurences\n",
      "merge 221/256 : (310, 266) -> 476 (b'est ') had 108 occurences\n",
      "merge 222/256 : (74, 270) -> 477 (b'Jan') had 108 occurences\n",
      "merge 223/256 : (50, 50) -> 478 (b'22') had 107 occurences\n",
      "merge 224/256 : (477, 370) -> 479 (b'January ') had 107 occurences\n",
      "merge 225/256 : (405, 366) -> 480 (b'ountr') had 106 occurences\n",
      "merge 226/256 : (382, 104) -> 481 (b'igh') had 106 occurences\n",
      "merge 227/256 : (300, 354) -> 482 (b'\". The ') had 106 occurences\n",
      "merge 228/256 : (359, 304) -> 483 (b', 2023.\\n ') had 106 occurences\n",
      "merge 229/256 : (49, 51) -> 484 (b'13') had 105 occurences\n",
      "merge 230/256 : (65, 108) -> 485 (b'Al') had 105 occurences\n",
      "merge 231/256 : (101, 116) -> 486 (b'et') had 105 occurences\n",
      "merge 232/256 : (310, 115) -> 487 (b'ess') had 103 occurences\n",
      "merge 233/256 : (452, 403) -> 488 (b'March ') had 103 occurences\n",
      "merge 234/256 : (117, 116) -> 489 (b'ut') had 102 occurences\n",
      "merge 235/256 : (119, 431) -> 490 (b'writ') had 101 occurences\n",
      "merge 236/256 : (108, 111) -> 491 (b'lo') had 99 occurences\n",
      "merge 237/256 : (115, 386) -> 492 (b'song') had 97 occurences\n",
      "merge 238/256 : (226, 128) -> 493 (b'\\xe2\\x80') had 97 occurences\n",
      "merge 239/256 : (271, 258) -> 494 (b'ard ') had 97 occurences\n",
      "merge 240/256 : (48, 32) -> 495 (b'0 ') had 97 occurences\n",
      "merge 241/256 : (117, 108) -> 496 (b'ul') had 96 occurences\n",
      "merge 242/256 : (50, 52) -> 497 (b'24') had 95 occurences\n",
      "merge 243/256 : (105, 262) -> 498 (b'is ') had 94 occurences\n",
      "merge 244/256 : (298, 99) -> 499 (b'tic') had 93 occurences\n",
      "merge 245/256 : (97, 103) -> 500 (b'ag') had 93 occurences\n",
      "merge 246/256 : (34, 32) -> 501 (b'\" ') had 93 occurences\n",
      "merge 247/256 : (65, 110) -> 502 (b'An') had 93 occurences\n",
      "merge 248/256 : (49, 56) -> 503 (b'18') had 93 occurences\n",
      "merge 249/256 : (102, 291) -> 504 (b'for') had 90 occurences\n",
      "merge 250/256 : (480, 273) -> 505 (b'ountry ') had 89 occurences\n",
      "merge 251/256 : (65, 420) -> 506 (b'Ame') had 88 occurences\n",
      "merge 252/256 : (506, 453) -> 507 (b'Americ') had 88 occurences\n",
      "merge 253/256 : (32, 84) -> 508 (b' T') had 88 occurences\n",
      "merge 254/256 : (115, 296) -> 509 (b'sing') had 87 occurences\n",
      "merge 255/256 : (119, 348) -> 510 (b'was ') had 86 occurences\n",
      "merge 256/256 : (49, 50) -> 511 (b'12') had 86 occurences\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(text,512,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "42659dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 256 -> Merged Bytes: b'e ' -> As String: 'e '\n",
      "Token ID: 257 -> Merged Bytes: b', ' -> As String: ', '\n",
      "Token ID: 258 -> Merged Bytes: b'd ' -> As String: 'd '\n",
      "Token ID: 259 -> Merged Bytes: b'. ' -> As String: '. '\n",
      "Token ID: 260 -> Merged Bytes: b'r ' -> As String: 'r '\n",
      "Token ID: 261 -> Merged Bytes: b'20' -> As String: '20'\n",
      "Token ID: 262 -> Merged Bytes: b's ' -> As String: 's '\n",
      "Token ID: 263 -> Merged Bytes: b'in' -> As String: 'in'\n",
      "Token ID: 264 -> Merged Bytes: b'on' -> As String: 'on'\n",
      "Token ID: 265 -> Merged Bytes: b'ri' -> As String: 'ri'\n",
      "Token ID: 266 -> Merged Bytes: b't ' -> As String: 't '\n",
      "Token ID: 267 -> Merged Bytes: b'th' -> As String: 'th'\n",
      "Token ID: 268 -> Merged Bytes: b'ed ' -> As String: 'ed '\n",
      "Token ID: 269 -> Merged Bytes: b', 20' -> As String: ', 20'\n",
      "Token ID: 270 -> Merged Bytes: b'an' -> As String: 'an'\n",
      "Token ID: 271 -> Merged Bytes: b'ar' -> As String: 'ar'\n",
      "Token ID: 272 -> Merged Bytes: b'er ' -> As String: 'er '\n",
      "Token ID: 273 -> Merged Bytes: b'y ' -> As String: 'y '\n",
      "Token ID: 274 -> Merged Bytes: b'al' -> As String: 'al'\n",
      "Token ID: 275 -> Merged Bytes: b'the ' -> As String: 'the '\n",
      "Token ID: 276 -> Merged Bytes: b'ved ' -> As String: 'ved '\n",
      "Token ID: 277 -> Merged Bytes: b'wi' -> As String: 'wi'\n",
      "Token ID: 278 -> Merged Bytes: b'er' -> As String: 'er'\n",
      "Token ID: 279 -> Merged Bytes: b'on ' -> As String: 'on '\n",
      "Token ID: 280 -> Merged Bytes: b'wif' -> As String: 'wif'\n",
      "Token ID: 281 -> Merged Bytes: b'Re' -> As String: 'Re'\n",
      "Token ID: 282 -> Merged Bytes: b'Swif' -> As String: 'Swif'\n",
      "Token ID: 283 -> Merged Bytes: b'or ' -> As String: 'or '\n",
      "Token ID: 284 -> Merged Bytes: b'ch' -> As String: 'ch'\n",
      "Token ID: 285 -> Merged Bytes: b', 201' -> As String: ', 201'\n",
      "Token ID: 286 -> Merged Bytes: b'om' -> As String: 'om'\n",
      "Token ID: 287 -> Merged Bytes: b'ber ' -> As String: 'ber '\n",
      "Token ID: 288 -> Merged Bytes: b' the ' -> As String: ' the '\n",
      "Token ID: 289 -> Merged Bytes: b'ay' -> As String: 'ay'\n",
      "Token ID: 290 -> Merged Bytes: b'en' -> As String: 'en'\n",
      "Token ID: 291 -> Merged Bytes: b'or' -> As String: 'or'\n",
      "Token ID: 292 -> Merged Bytes: b'al ' -> As String: 'al '\n",
      "Token ID: 293 -> Merged Bytes: b'em' -> As String: 'em'\n",
      "Token ID: 294 -> Merged Bytes: b'.\\n' -> As String: '.\n",
      "'\n",
      "Token ID: 295 -> Merged Bytes: b'rie' -> As String: 'rie'\n",
      "Token ID: 296 -> Merged Bytes: b'ing' -> As String: 'ing'\n",
      "Token ID: 297 -> Merged Bytes: b', 202' -> As String: ', 202'\n",
      "Token ID: 298 -> Merged Bytes: b'ti' -> As String: 'ti'\n",
      "Token ID: 299 -> Merged Bytes: b'ayl' -> As String: 'ayl'\n",
      "Token ID: 300 -> Merged Bytes: b'\". ' -> As String: '\". '\n",
      "Token ID: 301 -> Merged Bytes: b'll' -> As String: 'll'\n",
      "Token ID: 302 -> Merged Bytes: b'Tayl' -> As String: 'Tayl'\n",
      "Token ID: 303 -> Merged Bytes: b'trie' -> As String: 'trie'\n",
      "Token ID: 304 -> Merged Bytes: b'.\\n ' -> As String: '.\n",
      " '\n",
      "Token ID: 305 -> Merged Bytes: b'to' -> As String: 'to'\n",
      "Token ID: 306 -> Merged Bytes: b'. Re' -> As String: '. Re'\n",
      "Token ID: 307 -> Merged Bytes: b'. Retrie' -> As String: '. Retrie'\n",
      "Token ID: 308 -> Merged Bytes: b'. Retrieved ' -> As String: '. Retrieved '\n",
      "Token ID: 309 -> Merged Bytes: b'Taylor ' -> As String: 'Taylor '\n",
      "Token ID: 310 -> Merged Bytes: b'es' -> As String: 'es'\n",
      "Token ID: 311 -> Merged Bytes: b'Taylor Swif' -> As String: 'Taylor Swif'\n",
      "Token ID: 312 -> Merged Bytes: b'us' -> As String: 'us'\n",
      "Token ID: 313 -> Merged Bytes: b'rom' -> As String: 'rom'\n",
      "Token ID: 314 -> Merged Bytes: b'ember ' -> As String: 'ember '\n",
      "Token ID: 315 -> Merged Bytes: b'). ' -> As String: '). '\n",
      "Token ID: 316 -> Merged Bytes: b'Ar' -> As String: 'Ar'\n",
      "Token ID: 317 -> Merged Bytes: b'from' -> As String: 'from'\n",
      "Token ID: 318 -> Merged Bytes: b'). \"' -> As String: '). \"'\n",
      "Token ID: 319 -> Merged Bytes: b'and ' -> As String: 'and '\n",
      "Token ID: 320 -> Merged Bytes: b're' -> As String: 're'\n",
      "Token ID: 321 -> Merged Bytes: b'ou' -> As String: 'ou'\n",
      "Token ID: 322 -> Merged Bytes: b'ori' -> As String: 'ori'\n",
      "Token ID: 323 -> Merged Bytes: b'of' -> As String: 'of'\n",
      "Token ID: 324 -> Merged Bytes: b'gin' -> As String: 'gin'\n",
      "Token ID: 325 -> Merged Bytes: b'ing ' -> As String: 'ing '\n",
      "Token ID: 326 -> Merged Bytes: b'chi' -> As String: 'chi'\n",
      "Token ID: 327 -> Merged Bytes: b'] ' -> As String: '] '\n",
      "Token ID: 328 -> Merged Bytes: b'ginal ' -> As String: 'ginal '\n",
      "Token ID: 329 -> Merged Bytes: b'from the ' -> As String: 'from the '\n",
      "Token ID: 330 -> Merged Bytes: b'original ' -> As String: 'original '\n",
      "Token ID: 331 -> Merged Bytes: b'he ' -> As String: 'he '\n",
      "Token ID: 332 -> Merged Bytes: b'Archi' -> As String: 'Archi'\n",
      "Token ID: 333 -> Merged Bytes: b'Archived ' -> As String: 'Archived '\n",
      "Token ID: 334 -> Merged Bytes: b'from the original ' -> As String: 'from the original '\n",
      "Token ID: 335 -> Merged Bytes: b'Archived from the original ' -> As String: 'Archived from the original '\n",
      "Token ID: 336 -> Merged Bytes: b'Archived from the original on ' -> As String: 'Archived from the original on '\n",
      "Token ID: 337 -> Merged Bytes: b'. Archived from the original on ' -> As String: '. Archived from the original on '\n",
      "Token ID: 338 -> Merged Bytes: b'a ' -> As String: 'a '\n",
      "Token ID: 339 -> Merged Bytes: b'st' -> As String: 'st'\n",
      "Token ID: 340 -> Merged Bytes: b'ic' -> As String: 'ic'\n",
      "Token ID: 341 -> Merged Bytes: b'.[' -> As String: '.['\n",
      "Token ID: 342 -> Merged Bytes: b'ec' -> As String: 'ec'\n",
      "Token ID: 343 -> Merged Bytes: b'ill' -> As String: 'ill'\n",
      "Token ID: 344 -> Merged Bytes: b\"'s \" -> As String: ''s '\n",
      "Token ID: 345 -> Merged Bytes: b'Taylor Swift ' -> As String: 'Taylor Swift '\n",
      "Token ID: 346 -> Merged Bytes: b'ov' -> As String: 'ov'\n",
      "Token ID: 347 -> Merged Bytes: b'at' -> As String: 'at'\n",
      "Token ID: 348 -> Merged Bytes: b'as ' -> As String: 'as '\n",
      "Token ID: 349 -> Merged Bytes: b'es ' -> As String: 'es '\n",
      "Token ID: 350 -> Merged Bytes: b'Ju' -> As String: 'Ju'\n",
      "Token ID: 351 -> Merged Bytes: b'of ' -> As String: 'of '\n",
      "Token ID: 352 -> Merged Bytes: b'to ' -> As String: 'to '\n",
      "Token ID: 353 -> Merged Bytes: b'um' -> As String: 'um'\n",
      "Token ID: 354 -> Merged Bytes: b'The ' -> As String: 'The '\n",
      "Token ID: 355 -> Merged Bytes: b'ard' -> As String: 'ard'\n",
      "Token ID: 356 -> Merged Bytes: b'in ' -> As String: 'in '\n",
      "Token ID: 357 -> Merged Bytes: b'an ' -> As String: 'an '\n",
      "Token ID: 358 -> Merged Bytes: b'el' -> As String: 'el'\n",
      "Token ID: 359 -> Merged Bytes: b', 2023' -> As String: ', 2023'\n",
      "Token ID: 360 -> Merged Bytes: b'ary ' -> As String: 'ary '\n",
      "Token ID: 361 -> Merged Bytes: b'th ' -> As String: 'th '\n",
      "Token ID: 362 -> Merged Bytes: b'am' -> As String: 'am'\n",
      "Token ID: 363 -> Merged Bytes: b'ly ' -> As String: 'ly '\n",
      "Token ID: 364 -> Merged Bytes: b'op' -> As String: 'op'\n",
      "Token ID: 365 -> Merged Bytes: b'Taylor Swift' -> As String: 'Taylor Swift'\n",
      "Token ID: 366 -> Merged Bytes: b'tr' -> As String: 'tr'\n",
      "Token ID: 367 -> Merged Bytes: b'is' -> As String: 'is'\n",
      "Token ID: 368 -> Merged Bytes: b'her ' -> As String: 'her '\n",
      "Token ID: 369 -> Merged Bytes: b'o ' -> As String: 'o '\n",
      "Token ID: 370 -> Merged Bytes: b'uary ' -> As String: 'uary '\n",
      "Token ID: 371 -> Merged Bytes: b'Nov' -> As String: 'Nov'\n",
      "Token ID: 372 -> Merged Bytes: b'usic' -> As String: 'usic'\n",
      "Token ID: 373 -> Merged Bytes: b'November ' -> As String: 'November '\n",
      "Token ID: 374 -> Merged Bytes: b'ew' -> As String: 'ew'\n",
      "Token ID: 375 -> Merged Bytes: b'at ' -> As String: 'at '\n",
      "Token ID: 376 -> Merged Bytes: b'l ' -> As String: 'l '\n",
      "Token ID: 377 -> Merged Bytes: b': ' -> As String: ': '\n",
      "Token ID: 378 -> Merged Bytes: b'bo' -> As String: 'bo'\n",
      "Token ID: 379 -> Merged Bytes: b'Swift ' -> As String: 'Swift '\n",
      "Token ID: 380 -> Merged Bytes: b'Dec' -> As String: 'Dec'\n",
      "Token ID: 381 -> Merged Bytes: b'it' -> As String: 'it'\n",
      "Token ID: 382 -> Merged Bytes: b'ig' -> As String: 'ig'\n",
      "Token ID: 383 -> Merged Bytes: b'Bill' -> As String: 'Bill'\n",
      "Token ID: 384 -> Merged Bytes: b'10' -> As String: '10'\n",
      "Token ID: 385 -> Merged Bytes: b'as' -> As String: 'as'\n",
      "Token ID: 386 -> Merged Bytes: b'ong' -> As String: 'ong'\n",
      "Token ID: 387 -> Merged Bytes: b'Oc' -> As String: 'Oc'\n",
      "Token ID: 388 -> Merged Bytes: b'ati' -> As String: 'ati'\n",
      "Token ID: 389 -> Merged Bytes: b'St' -> As String: 'St'\n",
      "Token ID: 390 -> Merged Bytes: b'Octo' -> As String: 'Octo'\n",
      "Token ID: 391 -> Merged Bytes: b'October ' -> As String: 'October '\n",
      "Token ID: 392 -> Merged Bytes: b'ac' -> As String: 'ac'\n",
      "Token ID: 393 -> Merged Bytes: b'ow' -> As String: 'ow'\n",
      "Token ID: 394 -> Merged Bytes: b'December ' -> As String: 'December '\n",
      "Token ID: 395 -> Merged Bytes: b'Billbo' -> As String: 'Billbo'\n",
      "Token ID: 396 -> Merged Bytes: b'ad' -> As String: 'ad'\n",
      "Token ID: 397 -> Merged Bytes: b'le' -> As String: 'le'\n",
      "Token ID: 398 -> Merged Bytes: b'ur' -> As String: 'ur'\n",
      "Token ID: 399 -> Merged Bytes: b'for ' -> As String: 'for '\n",
      "Token ID: 400 -> Merged Bytes: b' (' -> As String: ' ('\n",
      "Token ID: 401 -> Merged Bytes: b', 2022' -> As String: ', 2022'\n",
      "Token ID: 402 -> Merged Bytes: b'ug' -> As String: 'ug'\n",
      "Token ID: 403 -> Merged Bytes: b'ch ' -> As String: 'ch '\n",
      "Token ID: 404 -> Merged Bytes: b'st ' -> As String: 'st '\n",
      "Token ID: 405 -> Merged Bytes: b'oun' -> As String: 'oun'\n",
      "Token ID: 406 -> Merged Bytes: b'bum' -> As String: 'bum'\n",
      "Token ID: 407 -> Merged Bytes: b'ol' -> As String: 'ol'\n",
      "Token ID: 408 -> Merged Bytes: b'ust ' -> As String: 'ust '\n",
      "Token ID: 409 -> Merged Bytes: b'eb' -> As String: 'eb'\n",
      "Token ID: 410 -> Merged Bytes: b'Ma' -> As String: 'Ma'\n",
      "Token ID: 411 -> Merged Bytes: b'July ' -> As String: 'July '\n",
      "Token ID: 412 -> Merged Bytes: b'). \"Taylor Swift ' -> As String: '). \"Taylor Swift '\n",
      "Token ID: 413 -> Merged Bytes: b'k ' -> As String: 'k '\n",
      "Token ID: 414 -> Merged Bytes: b'ers' -> As String: 'ers'\n",
      "Token ID: 415 -> Merged Bytes: b'][' -> As String: ']['\n",
      "Token ID: 416 -> Merged Bytes: b'Aug' -> As String: 'Aug'\n",
      "Token ID: 417 -> Merged Bytes: b'August ' -> As String: 'August '\n",
      "Token ID: 418 -> Merged Bytes: b'id' -> As String: 'id'\n",
      "Token ID: 419 -> Merged Bytes: b', 2021' -> As String: ', 2021'\n",
      "Token ID: 420 -> Merged Bytes: b'me' -> As String: 'me'\n",
      "Token ID: 421 -> Merged Bytes: b'ep' -> As String: 'ep'\n",
      "Token ID: 422 -> Merged Bytes: b'201' -> As String: '201'\n",
      "Token ID: 423 -> Merged Bytes: b'23' -> As String: '23'\n",
      "Token ID: 424 -> Merged Bytes: b', 2012' -> As String: ', 2012'\n",
      "Token ID: 425 -> Merged Bytes: b'ear' -> As String: 'ear'\n",
      "Token ID: 426 -> Merged Bytes: b', 2020' -> As String: ', 2020'\n",
      "Token ID: 427 -> Merged Bytes: b'In' -> As String: 'In'\n",
      "Token ID: 428 -> Merged Bytes: b'fi' -> As String: 'fi'\n",
      "Token ID: 429 -> Merged Bytes: b'ne ' -> As String: 'ne '\n",
      "Token ID: 430 -> Merged Bytes: b'Billboard' -> As String: 'Billboard'\n",
      "Token ID: 431 -> Merged Bytes: b'rit' -> As String: 'rit'\n",
      "Token ID: 432 -> Merged Bytes: b'hi' -> As String: 'hi'\n",
      "Token ID: 433 -> Merged Bytes: b'usic ' -> As String: 'usic '\n",
      "Token ID: 434 -> Merged Bytes: b'.\\n \"' -> As String: '.\n",
      " \"'\n",
      "Token ID: 435 -> Merged Bytes: b'New' -> As String: 'New'\n",
      "Token ID: 436 -> Merged Bytes: b'di' -> As String: 'di'\n",
      "Token ID: 437 -> Merged Bytes: b'Ap' -> As String: 'Ap'\n",
      "Token ID: 438 -> Merged Bytes: b', 2019' -> As String: ', 2019'\n",
      "Token ID: 439 -> Merged Bytes: b'ro' -> As String: 'ro'\n",
      "Token ID: 440 -> Merged Bytes: b\"' \" -> As String: '' '\n",
      "Token ID: 441 -> Merged Bytes: b's, ' -> As String: 's, '\n",
      "Token ID: 442 -> Merged Bytes: b'June ' -> As String: 'June '\n",
      "Token ID: 443 -> Merged Bytes: b'of the ' -> As String: 'of the '\n",
      "Token ID: 444 -> Merged Bytes: b'cor' -> As String: 'cor'\n",
      "Token ID: 445 -> Merged Bytes: b'21' -> As String: '21'\n",
      "Token ID: 446 -> Merged Bytes: b'19' -> As String: '19'\n",
      "Token ID: 447 -> Merged Bytes: b'im' -> As String: 'im'\n",
      "Token ID: 448 -> Merged Bytes: b'en ' -> As String: 'en '\n",
      "Token ID: 449 -> Merged Bytes: b'ebr' -> As String: 'ebr'\n",
      "Token ID: 450 -> Merged Bytes: b'ent' -> As String: 'ent'\n",
      "Token ID: 451 -> Merged Bytes: b'oll' -> As String: 'oll'\n",
      "Token ID: 452 -> Merged Bytes: b'Mar' -> As String: 'Mar'\n",
      "Token ID: 453 -> Merged Bytes: b'ric' -> As String: 'ric'\n",
      "Token ID: 454 -> Merged Bytes: b'with ' -> As String: 'with '\n",
      "Token ID: 455 -> Merged Bytes: b',[' -> As String: ',['\n",
      "Token ID: 456 -> Merged Bytes: b'Febr' -> As String: 'Febr'\n",
      "Token ID: 457 -> Merged Bytes: b'February ' -> As String: 'February '\n",
      "Token ID: 458 -> Merged Bytes: b\"Taylor Swift's \" -> As String: 'Taylor Swift's '\n",
      "Token ID: 459 -> Merged Bytes: b'\". Billboard' -> As String: '\". Billboard'\n",
      "Token ID: 460 -> Merged Bytes: b'ea' -> As String: 'ea'\n",
      "Token ID: 461 -> Merged Bytes: b', 2016' -> As String: ', 2016'\n",
      "Token ID: 462 -> Merged Bytes: b'ept' -> As String: 'ept'\n",
      "Token ID: 463 -> Merged Bytes: b'May ' -> As String: 'May '\n",
      "Token ID: 464 -> Merged Bytes: b', 2015' -> As String: ', 2015'\n",
      "Token ID: 465 -> Merged Bytes: b'Apri' -> As String: 'Apri'\n",
      "Token ID: 466 -> Merged Bytes: b'April ' -> As String: 'April '\n",
      "Token ID: 467 -> Merged Bytes: b'le ' -> As String: 'le '\n",
      "Token ID: 468 -> Merged Bytes: b'Aw' -> As String: 'Aw'\n",
      "Token ID: 469 -> Merged Bytes: b'ation' -> As String: 'ation'\n",
      "Token ID: 470 -> Merged Bytes: b'Sept' -> As String: 'Sept'\n",
      "Token ID: 471 -> Merged Bytes: b'September ' -> As String: 'September '\n",
      "Token ID: 472 -> Merged Bytes: b'ra' -> As String: 'ra'\n",
      "Token ID: 473 -> Merged Bytes: b'album' -> As String: 'album'\n",
      "Token ID: 474 -> Merged Bytes: b'Ch' -> As String: 'Ch'\n",
      "Token ID: 475 -> Merged Bytes: b've ' -> As String: 've '\n",
      "Token ID: 476 -> Merged Bytes: b'est ' -> As String: 'est '\n",
      "Token ID: 477 -> Merged Bytes: b'Jan' -> As String: 'Jan'\n",
      "Token ID: 478 -> Merged Bytes: b'22' -> As String: '22'\n",
      "Token ID: 479 -> Merged Bytes: b'January ' -> As String: 'January '\n",
      "Token ID: 480 -> Merged Bytes: b'ountr' -> As String: 'ountr'\n",
      "Token ID: 481 -> Merged Bytes: b'igh' -> As String: 'igh'\n",
      "Token ID: 482 -> Merged Bytes: b'\". The ' -> As String: '\". The '\n",
      "Token ID: 483 -> Merged Bytes: b', 2023.\\n ' -> As String: ', 2023.\n",
      " '\n",
      "Token ID: 484 -> Merged Bytes: b'13' -> As String: '13'\n",
      "Token ID: 485 -> Merged Bytes: b'Al' -> As String: 'Al'\n",
      "Token ID: 486 -> Merged Bytes: b'et' -> As String: 'et'\n",
      "Token ID: 487 -> Merged Bytes: b'ess' -> As String: 'ess'\n",
      "Token ID: 488 -> Merged Bytes: b'March ' -> As String: 'March '\n",
      "Token ID: 489 -> Merged Bytes: b'ut' -> As String: 'ut'\n",
      "Token ID: 490 -> Merged Bytes: b'writ' -> As String: 'writ'\n",
      "Token ID: 491 -> Merged Bytes: b'lo' -> As String: 'lo'\n",
      "Token ID: 492 -> Merged Bytes: b'song' -> As String: 'song'\n",
      "Token ID: 493 -> Merged Bytes: b'\\xe2\\x80' -> As String: 'ï¿½'\n",
      "Token ID: 494 -> Merged Bytes: b'ard ' -> As String: 'ard '\n",
      "Token ID: 495 -> Merged Bytes: b'0 ' -> As String: '0 '\n",
      "Token ID: 496 -> Merged Bytes: b'ul' -> As String: 'ul'\n",
      "Token ID: 497 -> Merged Bytes: b'24' -> As String: '24'\n",
      "Token ID: 498 -> Merged Bytes: b'is ' -> As String: 'is '\n",
      "Token ID: 499 -> Merged Bytes: b'tic' -> As String: 'tic'\n",
      "Token ID: 500 -> Merged Bytes: b'ag' -> As String: 'ag'\n",
      "Token ID: 501 -> Merged Bytes: b'\" ' -> As String: '\" '\n",
      "Token ID: 502 -> Merged Bytes: b'An' -> As String: 'An'\n",
      "Token ID: 503 -> Merged Bytes: b'18' -> As String: '18'\n",
      "Token ID: 504 -> Merged Bytes: b'for' -> As String: 'for'\n",
      "Token ID: 505 -> Merged Bytes: b'ountry ' -> As String: 'ountry '\n",
      "Token ID: 506 -> Merged Bytes: b'Ame' -> As String: 'Ame'\n",
      "Token ID: 507 -> Merged Bytes: b'Americ' -> As String: 'Americ'\n",
      "Token ID: 508 -> Merged Bytes: b' T' -> As String: ' T'\n",
      "Token ID: 509 -> Merged Bytes: b'sing' -> As String: 'sing'\n",
      "Token ID: 510 -> Merged Bytes: b'was ' -> As String: 'was '\n",
      "Token ID: 511 -> Merged Bytes: b'12' -> As String: '12'\n"
     ]
    }
   ],
   "source": [
    "# let's see our vocab\n",
    "for token, token_bytes in tokenizer.vocab.items():\n",
    "    if token>255:\n",
    "        print(f\"Token ID: {token} -> Merged Bytes: {token_bytes} -> As String: '{token_bytes.decode('utf-8', 'replace')}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "01279457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"merges\": {f\"{p0},{p1}\": idx for (p0,p1), idx in tokenizer.merges.items()},\n",
    "    \"vocab\": {str(idx): token_bytes.decode(\"latin-1\") for idx, token_bytes in tokenizer.vocab.items()},\n",
    "}\n",
    "with open(\"basic_tokenizer.json\", \"w\") as f:\n",
    "    json.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b951f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43221fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae3ed34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf4f0b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82c5007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "23409e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegexTokenizer(Tokenizer):\n",
    "    \n",
    "    def __init__(self, pattern = None):\n",
    "        super().__init__()\n",
    "        self.pattern = GPT4_SPLIT_PATTERN if pattern is None else pattern\n",
    "        self.compiled_pattern = re.compile(self.pattern)\n",
    "        self.special_token = {}\n",
    "        self.inverse_special_token = {}\n",
    "        \n",
    "    def train(self,text,vocab_size,verbose = False):\n",
    "        assert vocab_size >= 256\n",
    "        num_merges = vocab_size - 256\n",
    "        \n",
    "        # split the text up into text chunks \n",
    "        text_chunks = re.findall(self.compiled_pattern,text)\n",
    "        \n",
    "        # input text preprocessing\n",
    "        ids = [list(ch.encode(\"utf-8\")) for ch in text_chunks]\n",
    "        \n",
    "        # iteratively merge the most common pairs to create new tokens\n",
    "        merges = {}\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # idx -> bytes\n",
    "#         vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for i in range(num_merges):\n",
    "            # count the number of times every consecutive pair appears\n",
    "            stats = {}\n",
    "            for chunks_ids in ids:\n",
    "                # passing in stats will update it in place, adding up counts\n",
    "                get_stats(chunks_ids,stats)\n",
    "            \n",
    "            # find the pair with the highest count\n",
    "            pair = max(stats,key = stats.get)\n",
    "            # mint a new token: assign it the next available id\n",
    "            idx = 256 +i\n",
    "            # replace all occurrences of pair in ids with idx\n",
    "            ids = [merge(chunks_ids,pair,idx) for chunks_ids in ids]\n",
    "            # save the merge\n",
    "            merges[pair] = idx\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "            # prints\n",
    "            if verbose:\n",
    "                print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({vocab[idx]}) had {stats[pair]} occurrences\")\n",
    "            \n",
    "            # save class variables\n",
    "            self.merges = merges # used in encode()\n",
    "            self.vocab = vocab # used in decode()\n",
    "            \n",
    "    def register_special_tokens(self,special_tokens):\n",
    "        # special_tokens is a dictionary of str -> int\n",
    "        # example: {\"<|endoftext|>\": 100257}\n",
    "        self.special_tokens = special_tokens\n",
    "        self.inverse_special_tokens = {v:k for k,v in special_tokens.items()}\n",
    "        \n",
    "    def decode(self,ids):\n",
    "        # given ids (list of integers), return Python string\n",
    "        part_bytes = []\n",
    "        for idx in ids:\n",
    "            if idx in self.vocab:\n",
    "                part_bytes.append(self.vocab[idx])\n",
    "            elif idx in self.inverse_special_tokens:\n",
    "                part_bytes.append(self.inverse_special_tokens[idx].encode(\"utf-8\"))\n",
    "            else:\n",
    "                raise ValueError(f\"invalid token id: {idx}\")\n",
    "                \n",
    "        text_bytes = b\"\".join(part_bytes)\n",
    "        text = text_bytes.decode(\"utf-8\",errors = \"replace\")\n",
    "        return text\n",
    "    \n",
    "    def _encode_chunk(self,text_bytes):\n",
    "        # first, convert all bytes to integers in range 0..255\n",
    "        ids = list(text_bytes)\n",
    "        while len(ids) >= 2:\n",
    "            # find the pair with the lowest merge index\n",
    "            stats = get_stats(ids)\n",
    "            pair = min(stats, key = lambda p: self.merges.get(p,float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(ids,pair,idx)\n",
    "            \n",
    "        return ids\n",
    "    \n",
    "    def encode_ordinary(self,text):\n",
    "        # \"\"\"Encoding that ignores any special tokens.\"\"\"\n",
    "        # split text into chunks of text by categories defined in regex pattern\n",
    "        text_chunks = re.findall(self.compiled_pattern,text)\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for chunks in text_chunks:\n",
    "            chunks_bytes = chunks.encode(\"utf-8\")\n",
    "            chunks_ids = self._encode_chunk(chunks_bytes)\n",
    "            ids.extend(chunks_ids)\n",
    "        return ids\n",
    "    \n",
    "    def encode(self,text,allowed_special = \"none_raise\"):\n",
    "        special = None\n",
    "        if allowed_special == \"all\":\n",
    "            special = self.special_tokens\n",
    "            \n",
    "        elif allowed_special == \"none\":\n",
    "            special = {}\n",
    "        elif allowed_special == \"none_raise\":\n",
    "            special = {}\n",
    "            assert all(token not in text for token in self.special_tokens)\n",
    "        elif isinstance(allowed_special,set):\n",
    "            special = {k:v for k,v in self.special_tokens.items() if k in allowed_special}\n",
    "        else:\n",
    "            raise ValueError(f\"allowed_special={allowed_special} not understood\")\n",
    "        if not special:\n",
    "            # shortcut: if no special tokens, just use the ordinary encoding\n",
    "            return self.encode_ordinary(text)\n",
    "        \n",
    "        special_pattern = \"(\" + \"|\".join(re.escape(k) for k in special) + \")\"\n",
    "        special_chunks = re.split(special_pattern,text)\n",
    "        # now all the special characters are separated from the rest of the text\n",
    "        # all chunks of text are encoded separately, then results are joined\n",
    "        ids = []\n",
    "        for part in special_chunks:\n",
    "            if part in special:\n",
    "                \n",
    "                 # this is a special token, encode it separately as a special case\n",
    "                ids.append(special[part])\n",
    "            else:\n",
    "                # this is an ordinary sequence, encode it normally\n",
    "                ids.extend(self.encode_ordinary(part))\n",
    "        return ids\n",
    "                    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2915e71a",
   "metadata": {},
   "source": [
    "encode_ordinary(self, text),_encode_chunk(self, text_bytes) =   handle the encoding of normal text (without any special tokens)\n",
    "\n",
    "encode_ordinary first splits the input text into chunks using the regex, just like in training.\n",
    "For each chunk, it calls _encode_chunk.\n",
    "_encode_chunk takes the bytes of a single chunk and repeatedly applies the merge rules it learned during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "867b1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = RegexTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ca2b13d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/256: (101, 114) -> 256 (b'er') had 2359 occurrences\n",
      "merge 2/256: (50, 48) -> 257 (b'20') had 2187 occurrences\n",
      "merge 3/256: (111, 114) -> 258 (b'or') had 2076 occurrences\n",
      "merge 4/256: (105, 110) -> 259 (b'in') had 2006 occurrences\n",
      "merge 5/256: (101, 100) -> 260 (b'ed') had 1876 occurrences\n",
      "merge 6/256: (32, 116) -> 261 (b' t') had 1824 occurrences\n",
      "merge 7/256: (111, 110) -> 262 (b'on') had 1815 occurrences\n",
      "merge 8/256: (104, 101) -> 263 (b'he') had 1772 occurrences\n",
      "merge 9/256: (32, 83) -> 264 (b' S') had 1633 occurrences\n",
      "merge 10/256: (97, 114) -> 265 (b'ar') had 1519 occurrences\n",
      "merge 11/256: (97, 110) -> 266 (b'an') had 1487 occurrences\n",
      "merge 12/256: (32, 65) -> 267 (b' A') had 1335 occurrences\n",
      "merge 13/256: (261, 263) -> 268 (b' the') had 1169 occurrences\n",
      "merge 14/256: (97, 108) -> 269 (b'al') had 1164 occurrences\n",
      "merge 15/256: (114, 105) -> 270 (b'ri') had 1156 occurrences\n",
      "merge 16/256: (118, 260) -> 271 (b'ved') had 1104 occurrences\n",
      "merge 17/256: (115, 116) -> 272 (b'st') had 1089 occurrences\n",
      "merge 18/256: (119, 105) -> 273 (b'wi') had 1049 occurrences\n",
      "merge 19/256: (32, 82) -> 274 (b' R') had 1045 occurrences\n",
      "merge 20/256: (257, 49) -> 275 (b'201') had 981 occurrences\n",
      "merge 21/256: (32, 102) -> 276 (b' f') had 967 occurrences\n",
      "merge 22/256: (257, 50) -> 277 (b'202') had 952 occurrences\n",
      "merge 23/256: (32, 84) -> 278 (b' T') had 934 occurrences\n",
      "merge 24/256: (102, 116) -> 279 (b'ft') had 934 occurrences\n",
      "merge 25/256: (97, 121) -> 280 (b'ay') had 900 occurrences\n",
      "merge 26/256: (32, 34) -> 281 (b' \"') had 882 occurrences\n",
      "merge 27/256: (273, 279) -> 282 (b'wift') had 870 occurrences\n",
      "merge 28/256: (101, 116) -> 283 (b'et') had 852 occurrences\n",
      "merge 29/256: (264, 282) -> 284 (b' Swift') had 817 occurrences\n",
      "merge 30/256: (99, 104) -> 285 (b'ch') had 797 occurrences\n",
      "merge 31/256: (98, 256) -> 286 (b'ber') had 797 occurrences\n",
      "merge 32/256: (97, 116) -> 287 (b'at') had 790 occurrences\n",
      "merge 33/256: (111, 109) -> 288 (b'om') had 789 occurrences\n",
      "merge 34/256: (101, 115) -> 289 (b'es') had 743 occurrences\n",
      "merge 35/256: (101, 110) -> 290 (b'en') had 724 occurrences\n",
      "merge 36/256: (101, 109) -> 291 (b'em') had 699 occurrences\n",
      "merge 37/256: (34, 46) -> 292 (b'\".') had 693 occurrences\n",
      "merge 38/256: (32, 40) -> 293 (b' (') had 685 occurrences\n",
      "merge 39/256: (46, 10) -> 294 (b'.\\n') had 684 occurrences\n",
      "merge 40/256: (259, 103) -> 295 (b'ing') had 684 occurrences\n",
      "merge 41/256: (108, 258) -> 296 (b'lor') had 680 occurrences\n",
      "merge 42/256: (32, 77) -> 297 (b' M') had 662 occurrences\n",
      "merge 43/256: (105, 103) -> 298 (b'ig') had 655 occurrences\n",
      "merge 44/256: (32, 262) -> 299 (b' on') had 654 occurrences\n",
      "merge 45/256: (280, 296) -> 300 (b'aylor') had 650 occurrences\n",
      "merge 46/256: (108, 108) -> 301 (b'll') had 649 occurrences\n",
      "merge 47/256: (270, 101) -> 302 (b'rie') had 646 occurrences\n",
      "merge 48/256: (274, 283) -> 303 (b' Ret') had 643 occurrences\n",
      "merge 49/256: (303, 302) -> 304 (b' Retrie') had 639 occurrences\n",
      "merge 50/256: (304, 271) -> 305 (b' Retrieved') had 639 occurrences\n",
      "merge 51/256: (32, 115) -> 306 (b' s') had 609 occurrences\n",
      "merge 52/256: (105, 99) -> 307 (b'ic') had 593 occurrences\n",
      "merge 53/256: (266, 100) -> 308 (b'and') had 573 occurrences\n",
      "merge 54/256: (111, 117) -> 309 (b'ou') had 555 occurrences\n",
      "merge 55/256: (101, 99) -> 310 (b'ec') had 554 occurrences\n",
      "merge 56/256: (32, 97) -> 311 (b' a') had 553 occurrences\n",
      "merge 57/256: (41, 46) -> 312 (b').') had 533 occurrences\n",
      "merge 58/256: (114, 288) -> 313 (b'rom') had 532 occurrences\n",
      "merge 59/256: (32, 66) -> 314 (b' B') had 530 occurrences\n",
      "merge 60/256: (291, 286) -> 315 (b'ember') had 529 occurrences\n",
      "merge 61/256: (32, 111) -> 316 (b' o') had 527 occurrences\n",
      "merge 62/256: (276, 313) -> 317 (b' from') had 503 occurrences\n",
      "merge 63/256: (267, 114) -> 318 (b' Ar') had 495 occurrences\n",
      "merge 64/256: (32, 308) -> 319 (b' and') had 480 occurrences\n",
      "merge 65/256: (32, 67) -> 320 (b' C') had 472 occurrences\n",
      "merge 66/256: (32, 78) -> 321 (b' N') had 472 occurrences\n",
      "merge 67/256: (32, 258) -> 322 (b' or') had 465 occurrences\n",
      "merge 68/256: (285, 105) -> 323 (b'chi') had 458 occurrences\n",
      "merge 69/256: (32, 74) -> 324 (b' J') had 457 occurrences\n",
      "merge 70/256: (259, 269) -> 325 (b'inal') had 456 occurrences\n",
      "merge 71/256: (322, 298) -> 326 (b' orig') had 448 occurrences\n",
      "merge 72/256: (326, 325) -> 327 (b' original') had 446 occurrences\n",
      "merge 73/256: (318, 323) -> 328 (b' Archi') had 440 occurrences\n",
      "merge 74/256: (328, 271) -> 329 (b' Archived') had 440 occurrences\n",
      "merge 75/256: (316, 102) -> 330 (b' of') had 439 occurrences\n",
      "merge 76/256: (32, 104) -> 331 (b' h') had 432 occurrences\n",
      "merge 77/256: (32, 259) -> 332 (b' in') had 404 occurrences\n",
      "merge 78/256: (114, 101) -> 333 (b're') had 403 occurrences\n",
      "merge 79/256: (84, 300) -> 334 (b'Taylor') had 392 occurrences\n",
      "merge 80/256: (105, 116) -> 335 (b'it') had 386 occurrences\n",
      "merge 81/256: (97, 115) -> 336 (b'as') had 384 occurrences\n",
      "merge 82/256: (32, 112) -> 337 (b' p') had 381 occurrences\n",
      "merge 83/256: (105, 262) -> 338 (b'ion') had 381 occurrences\n",
      "merge 84/256: (32, 68) -> 339 (b' D') had 380 occurrences\n",
      "merge 85/256: (32, 119) -> 340 (b' w') had 380 occurrences\n",
      "merge 86/256: (265, 100) -> 341 (b'ard') had 374 occurrences\n",
      "merge 87/256: (105, 301) -> 342 (b'ill') had 371 occurrences\n",
      "merge 88/256: (39, 115) -> 343 (b\"'s\") had 368 occurrences\n",
      "merge 89/256: (32, 109) -> 344 (b' m') had 359 occurrences\n",
      "merge 90/256: (32, 70) -> 345 (b' F') had 356 occurrences\n",
      "merge 91/256: (32, 87) -> 346 (b' W') had 353 occurrences\n",
      "merge 92/256: (108, 101) -> 347 (b'le') had 345 occurrences\n",
      "merge 93/256: (261, 111) -> 348 (b' to') had 340 occurrences\n",
      "merge 94/256: (32, 99) -> 349 (b' c') had 340 occurrences\n",
      "merge 95/256: (46, 91) -> 350 (b'.[') had 332 occurrences\n",
      "merge 96/256: (111, 118) -> 351 (b'ov') had 317 occurrences\n",
      "merge 97/256: (108, 121) -> 352 (b'ly') had 314 occurrences\n",
      "merge 98/256: (117, 115) -> 353 (b'us') had 312 occurrences\n",
      "merge 99/256: (32, 72) -> 354 (b' H') had 310 occurrences\n",
      "merge 100/256: (105, 115) -> 355 (b'is') had 307 occurrences\n",
      "merge 101/256: (32, 80) -> 356 (b' P') had 302 occurrences\n",
      "merge 102/256: (116, 104) -> 357 (b'th') had 290 occurrences\n",
      "merge 103/256: (99, 116) -> 358 (b'ct') had 282 occurrences\n",
      "merge 104/256: (117, 109) -> 359 (b'um') had 281 occurrences\n",
      "merge 105/256: (32, 98) -> 360 (b' b') had 280 occurrences\n",
      "merge 106/256: (32, 71) -> 361 (b' G') had 277 occurrences\n",
      "merge 107/256: (265, 121) -> 362 (b'ary') had 267 occurrences\n",
      "merge 108/256: (32, 73) -> 363 (b' I') had 264 occurrences\n",
      "merge 109/256: (278, 300) -> 364 (b' Taylor') had 255 occurrences\n",
      "merge 110/256: (105, 272) -> 365 (b'ist') had 248 occurrences\n",
      "merge 111/256: (32, 100) -> 366 (b' d') had 247 occurrences\n",
      "merge 112/256: (97, 109) -> 367 (b'am') had 247 occurrences\n",
      "merge 113/256: (32, 79) -> 368 (b' O') had 247 occurrences\n",
      "merge 114/256: (111, 112) -> 369 (b'op') had 243 occurrences\n",
      "merge 115/256: (324, 117) -> 370 (b' Ju') had 239 occurrences\n",
      "merge 116/256: (331, 256) -> 371 (b' her') had 235 occurrences\n",
      "merge 117/256: (32, 76) -> 372 (b' L') had 228 occurrences\n",
      "merge 118/256: (117, 272) -> 373 (b'ust') had 228 occurrences\n",
      "merge 119/256: (97, 100) -> 374 (b'ad') had 225 occurrences\n",
      "merge 120/256: (278, 263) -> 375 (b' The') had 225 occurrences\n",
      "merge 121/256: (117, 362) -> 376 (b'uary') had 225 occurrences\n",
      "merge 122/256: (290, 116) -> 377 (b'ent') had 223 occurrences\n",
      "merge 123/256: (353, 307) -> 378 (b'usic') had 221 occurrences\n",
      "merge 124/256: (351, 315) -> 379 (b'ovember') had 221 occurrences\n",
      "merge 125/256: (101, 119) -> 380 (b'ew') had 216 occurrences\n",
      "merge 126/256: (256, 115) -> 381 (b'ers') had 215 occurrences\n",
      "merge 127/256: (265, 116) -> 382 (b'art') had 204 occurrences\n",
      "merge 128/256: (105, 100) -> 383 (b'id') had 204 occurrences\n",
      "merge 129/256: (32, 69) -> 384 (b' E') had 203 occurrences\n",
      "merge 130/256: (101, 108) -> 385 (b'el') had 203 occurrences\n",
      "merge 131/256: (262, 103) -> 386 (b'ong') had 202 occurrences\n",
      "merge 132/256: (105, 109) -> 387 (b'im') had 202 occurrences\n",
      "merge 133/256: (111, 286) -> 388 (b'ober') had 202 occurrences\n",
      "merge 134/256: (101, 112) -> 389 (b'ep') had 201 occurrences\n",
      "merge 135/256: (276, 258) -> 390 (b' for') had 199 occurrences\n",
      "merge 136/256: (358, 388) -> 391 (b'ctober') had 198 occurrences\n",
      "merge 137/256: (98, 111) -> 392 (b'bo') had 197 occurrences\n",
      "merge 138/256: (314, 342) -> 393 (b' Bill') had 196 occurrences\n",
      "merge 139/256: (310, 315) -> 394 (b'ecember') had 195 occurrences\n",
      "merge 140/256: (264, 116) -> 395 (b' St') had 195 occurrences\n",
      "merge 141/256: (392, 341) -> 396 (b'board') had 192 occurrences\n",
      "merge 142/256: (111, 119) -> 397 (b'ow') had 186 occurrences\n",
      "merge 143/256: (117, 103) -> 398 (b'ug') had 185 occurrences\n",
      "merge 144/256: (111, 116) -> 399 (b'ot') had 184 occurrences\n",
      "merge 145/256: (393, 396) -> 400 (b' Billboard') had 184 occurrences\n",
      "merge 146/256: (49, 48) -> 401 (b'10') had 183 occurrences\n",
      "merge 147/256: (110, 116) -> 402 (b'nt') had 182 occurrences\n",
      "merge 148/256: (110, 101) -> 403 (b'ne') had 182 occurrences\n",
      "merge 149/256: (101, 265) -> 404 (b'ear') had 179 occurrences\n",
      "merge 150/256: (309, 114) -> 405 (b'our') had 179 occurrences\n",
      "merge 151/256: (270, 116) -> 406 (b'rit') had 177 occurrences\n",
      "merge 152/256: (105, 114) -> 407 (b'ir') had 176 occurrences\n",
      "merge 153/256: (98, 359) -> 408 (b'bum') had 172 occurrences\n",
      "merge 154/256: (117, 114) -> 409 (b'ur') had 171 occurrences\n",
      "merge 155/256: (287, 338) -> 410 (b'ation') had 171 occurrences\n",
      "merge 156/256: (257, 48) -> 411 (b'200') had 171 occurrences\n",
      "merge 157/256: (111, 108) -> 412 (b'ol') had 168 occurrences\n",
      "merge 158/256: (289, 115) -> 413 (b'ess') had 166 occurrences\n",
      "merge 159/256: (32, 114) -> 414 (b' r') had 164 occurrences\n",
      "merge 160/256: (93, 91) -> 415 (b'][') had 164 occurrences\n",
      "merge 161/256: (398, 373) -> 416 (b'ugust') had 164 occurrences\n",
      "merge 162/256: (32, 86) -> 417 (b' V') had 161 occurrences\n",
      "merge 163/256: (32, 39) -> 418 (b\" '\") had 160 occurrences\n",
      "merge 164/256: (101, 98) -> 419 (b'eb') had 157 occurrences\n",
      "merge 165/256: (32, 269) -> 420 (b' al') had 153 occurrences\n",
      "merge 166/256: (105, 118) -> 421 (b'iv') had 152 occurrences\n",
      "merge 167/256: (32, 89) -> 422 (b' Y') had 152 occurrences\n",
      "merge 168/256: (117, 116) -> 423 (b'ut') had 151 occurrences\n",
      "merge 169/256: (32, 273) -> 424 (b' wi') had 150 occurrences\n",
      "merge 170/256: (114, 121) -> 425 (b'ry') had 149 occurrences\n",
      "merge 171/256: (101, 272) -> 426 (b'est') had 144 occurrences\n",
      "merge 172/256: (424, 357) -> 427 (b' with') had 143 occurrences\n",
      "merge 173/256: (114, 97) -> 428 (b'ra') had 141 occurrences\n",
      "merge 174/256: (339, 394) -> 429 (b' December') had 140 occurrences\n",
      "merge 175/256: (321, 379) -> 430 (b' November') had 138 occurrences\n",
      "merge 176/256: (32, 287) -> 431 (b' at') had 133 occurrences\n",
      "merge 177/256: (32, 333) -> 432 (b' re') had 130 occurrences\n",
      "merge 178/256: (370, 352) -> 433 (b' July') had 129 occurrences\n",
      "merge 179/256: (261, 104) -> 434 (b' th') had 127 occurrences\n",
      "merge 180/256: (32, 110) -> 435 (b' n') had 127 occurrences\n",
      "merge 181/256: (267, 416) -> 436 (b' August') had 127 occurrences\n",
      "merge 182/256: (258, 100) -> 437 (b'ord') had 126 occurrences\n",
      "merge 183/256: (119, 341) -> 438 (b'ward') had 125 occurrences\n",
      "merge 184/256: (49, 57) -> 439 (b'19') had 125 occurrences\n",
      "merge 185/256: (32, 75) -> 440 (b' K') had 125 occurrences\n",
      "merge 186/256: (368, 391) -> 441 (b' October') had 125 occurrences\n",
      "merge 187/256: (32, 108) -> 442 (b' l') had 123 occurrences\n",
      "merge 188/256: (111, 301) -> 443 (b'oll') had 122 occurrences\n",
      "merge 189/256: (112, 270) -> 444 (b'pri') had 122 occurrences\n",
      "merge 190/256: (117, 108) -> 445 (b'ul') had 121 occurrences\n",
      "merge 191/256: (389, 116) -> 446 (b'ept') had 121 occurrences\n",
      "merge 192/256: (297, 378) -> 447 (b' Music') had 119 occurrences\n",
      "merge 193/256: (32, 85) -> 448 (b' U') had 119 occurrences\n",
      "merge 194/256: (111, 115) -> 449 (b'os') had 119 occurrences\n",
      "merge 195/256: (419, 114) -> 450 (b'ebr') had 119 occurrences\n",
      "merge 196/256: (311, 115) -> 451 (b' as') had 118 occurrences\n",
      "merge 197/256: (110, 100) -> 452 (b'nd') had 118 occurrences\n",
      "merge 198/256: (44, 91) -> 453 (b',[') had 118 occurrences\n",
      "merge 199/256: (450, 376) -> 454 (b'ebruary') had 118 occurrences\n",
      "merge 200/256: (321, 380) -> 455 (b' New') had 117 occurrences\n",
      "merge 201/256: (32, 272) -> 456 (b' st') had 116 occurrences\n",
      "merge 202/256: (111, 100) -> 457 (b'od') had 115 occurrences\n",
      "merge 203/256: (97, 107) -> 458 (b'ak') had 115 occurrences\n",
      "merge 204/256: (444, 108) -> 459 (b'pril') had 115 occurrences\n",
      "merge 205/256: (309, 402) -> 460 (b'ount') had 114 occurrences\n",
      "merge 206/256: (320, 104) -> 461 (b' Ch') had 114 occurrences\n",
      "merge 207/256: (99, 101) -> 462 (b'ce') had 113 occurrences\n",
      "merge 208/256: (446, 315) -> 463 (b'eptember') had 112 occurrences\n",
      "merge 209/256: (420, 408) -> 464 (b' album') had 110 occurrences\n",
      "merge 210/256: (105, 108) -> 465 (b'il') had 109 occurrences\n",
      "merge 211/256: (267, 438) -> 466 (b' Award') had 108 occurrences\n",
      "merge 212/256: (310, 437) -> 467 (b'ecord') had 108 occurrences\n",
      "merge 213/256: (266, 376) -> 468 (b'anuary') had 107 occurrences\n",
      "merge 214/256: (49, 51) -> 469 (b'13') had 106 occurrences\n",
      "merge 215/256: (460, 425) -> 470 (b'ountry') had 106 occurrences\n",
      "merge 216/256: (109, 256) -> 471 (b'mer') had 105 occurrences\n",
      "merge 217/256: (262, 101) -> 472 (b'one') had 105 occurrences\n",
      "merge 218/256: (32, 103) -> 473 (b' g') had 104 occurrences\n",
      "merge 219/256: (50, 49) -> 474 (b'21') had 103 occurrences\n",
      "merge 220/256: (265, 285) -> 475 (b'arch') had 103 occurrences\n",
      "merge 221/256: (111, 99) -> 476 (b'oc') had 101 occurrences\n",
      "merge 222/256: (310, 116) -> 477 (b'ect') had 100 occurrences\n",
      "merge 223/256: (105, 97) -> 478 (b'ia') had 98 occurrences\n",
      "merge 224/256: (118, 256) -> 479 (b'ver') had 98 occurrences\n",
      "merge 225/256: (226, 128) -> 480 (b'\\xe2\\x80') had 97 occurrences\n",
      "merge 226/256: (264, 263) -> 481 (b' She') had 96 occurrences\n",
      "merge 227/256: (297, 280) -> 482 (b' May') had 96 occurrences\n",
      "merge 228/256: (267, 108) -> 483 (b' Al') had 95 occurrences\n",
      "merge 229/256: (50, 51) -> 484 (b'23') had 95 occurrences\n",
      "merge 230/256: (370, 403) -> 485 (b' June') had 95 occurrences\n",
      "merge 231/256: (344, 378) -> 486 (b' music') had 94 occurrences\n",
      "merge 232/256: (49, 56) -> 487 (b'18') had 93 occurrences\n",
      "merge 233/256: (278, 387) -> 488 (b' Tim') had 91 occurrences\n",
      "merge 234/256: (421, 101) -> 489 (b'ive') had 90 occurrences\n",
      "merge 235/256: (345, 454) -> 490 (b' February') had 90 occurrences\n",
      "merge 236/256: (101, 101) -> 491 (b'ee') had 89 occurrences\n",
      "merge 237/256: (105, 266) -> 492 (b'ian') had 89 occurrences\n",
      "merge 238/256: (50, 52) -> 493 (b'24') had 89 occurrences\n",
      "merge 239/256: (471, 307) -> 494 (b'meric') had 88 occurrences\n",
      "merge 240/256: (49, 50) -> 495 (b'12') had 87 occurrences\n",
      "merge 241/256: (264, 463) -> 496 (b' September') had 87 occurrences\n",
      "merge 242/256: (306, 295) -> 497 (b' sing') had 86 occurrences\n",
      "merge 243/256: (32, 118) -> 498 (b' v') had 86 occurrences\n",
      "merge 244/256: (340, 336) -> 499 (b' was') had 86 occurrences\n",
      "merge 245/256: (267, 459) -> 500 (b' April') had 86 occurrences\n",
      "merge 246/256: (49, 55) -> 501 (b'17') had 84 occurrences\n",
      "merge 247/256: (50, 50) -> 502 (b'22') had 84 occurrences\n",
      "merge 248/256: (111, 103) -> 503 (b'og') had 83 occurrences\n",
      "merge 249/256: (119, 406) -> 504 (b'writ') had 83 occurrences\n",
      "merge 250/256: (78, 379) -> 505 (b'November') had 83 occurrences\n",
      "merge 251/256: (306, 386) -> 506 (b' song') had 82 occurrences\n",
      "merge 252/256: (49, 52) -> 507 (b'14') had 82 occurrences\n",
      "merge 253/256: (298, 104) -> 508 (b'igh') had 82 occurrences\n",
      "merge 254/256: (108, 100) -> 509 (b'ld') had 82 occurrences\n",
      "merge 255/256: (93, 10) -> 510 (b']\\n') had 82 occurrences\n",
      "merge 256/256: (306, 263) -> 511 (b' she') had 82 occurrences\n"
     ]
    }
   ],
   "source": [
    "tokenizer2.train(text,512,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0da62cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 256 -> Merged Bytes: b'er' -> As String: 'er'\n",
      "Token ID: 257 -> Merged Bytes: b'20' -> As String: '20'\n",
      "Token ID: 258 -> Merged Bytes: b'or' -> As String: 'or'\n",
      "Token ID: 259 -> Merged Bytes: b'in' -> As String: 'in'\n",
      "Token ID: 260 -> Merged Bytes: b'ed' -> As String: 'ed'\n",
      "Token ID: 261 -> Merged Bytes: b' t' -> As String: ' t'\n",
      "Token ID: 262 -> Merged Bytes: b'on' -> As String: 'on'\n",
      "Token ID: 263 -> Merged Bytes: b'he' -> As String: 'he'\n",
      "Token ID: 264 -> Merged Bytes: b' S' -> As String: ' S'\n",
      "Token ID: 265 -> Merged Bytes: b'ar' -> As String: 'ar'\n",
      "Token ID: 266 -> Merged Bytes: b'an' -> As String: 'an'\n",
      "Token ID: 267 -> Merged Bytes: b' A' -> As String: ' A'\n",
      "Token ID: 268 -> Merged Bytes: b' the' -> As String: ' the'\n",
      "Token ID: 269 -> Merged Bytes: b'al' -> As String: 'al'\n",
      "Token ID: 270 -> Merged Bytes: b'ri' -> As String: 'ri'\n",
      "Token ID: 271 -> Merged Bytes: b'ved' -> As String: 'ved'\n",
      "Token ID: 272 -> Merged Bytes: b'st' -> As String: 'st'\n",
      "Token ID: 273 -> Merged Bytes: b'wi' -> As String: 'wi'\n",
      "Token ID: 274 -> Merged Bytes: b' R' -> As String: ' R'\n",
      "Token ID: 275 -> Merged Bytes: b'201' -> As String: '201'\n",
      "Token ID: 276 -> Merged Bytes: b' f' -> As String: ' f'\n",
      "Token ID: 277 -> Merged Bytes: b'202' -> As String: '202'\n",
      "Token ID: 278 -> Merged Bytes: b' T' -> As String: ' T'\n",
      "Token ID: 279 -> Merged Bytes: b'ft' -> As String: 'ft'\n",
      "Token ID: 280 -> Merged Bytes: b'ay' -> As String: 'ay'\n",
      "Token ID: 281 -> Merged Bytes: b' \"' -> As String: ' \"'\n",
      "Token ID: 282 -> Merged Bytes: b'wift' -> As String: 'wift'\n",
      "Token ID: 283 -> Merged Bytes: b'et' -> As String: 'et'\n",
      "Token ID: 284 -> Merged Bytes: b' Swift' -> As String: ' Swift'\n",
      "Token ID: 285 -> Merged Bytes: b'ch' -> As String: 'ch'\n",
      "Token ID: 286 -> Merged Bytes: b'ber' -> As String: 'ber'\n",
      "Token ID: 287 -> Merged Bytes: b'at' -> As String: 'at'\n",
      "Token ID: 288 -> Merged Bytes: b'om' -> As String: 'om'\n",
      "Token ID: 289 -> Merged Bytes: b'es' -> As String: 'es'\n",
      "Token ID: 290 -> Merged Bytes: b'en' -> As String: 'en'\n",
      "Token ID: 291 -> Merged Bytes: b'em' -> As String: 'em'\n",
      "Token ID: 292 -> Merged Bytes: b'\".' -> As String: '\".'\n",
      "Token ID: 293 -> Merged Bytes: b' (' -> As String: ' ('\n",
      "Token ID: 294 -> Merged Bytes: b'.\\n' -> As String: '.\n",
      "'\n",
      "Token ID: 295 -> Merged Bytes: b'ing' -> As String: 'ing'\n",
      "Token ID: 296 -> Merged Bytes: b'lor' -> As String: 'lor'\n",
      "Token ID: 297 -> Merged Bytes: b' M' -> As String: ' M'\n",
      "Token ID: 298 -> Merged Bytes: b'ig' -> As String: 'ig'\n",
      "Token ID: 299 -> Merged Bytes: b' on' -> As String: ' on'\n",
      "Token ID: 300 -> Merged Bytes: b'aylor' -> As String: 'aylor'\n",
      "Token ID: 301 -> Merged Bytes: b'll' -> As String: 'll'\n",
      "Token ID: 302 -> Merged Bytes: b'rie' -> As String: 'rie'\n",
      "Token ID: 303 -> Merged Bytes: b' Ret' -> As String: ' Ret'\n",
      "Token ID: 304 -> Merged Bytes: b' Retrie' -> As String: ' Retrie'\n",
      "Token ID: 305 -> Merged Bytes: b' Retrieved' -> As String: ' Retrieved'\n",
      "Token ID: 306 -> Merged Bytes: b' s' -> As String: ' s'\n",
      "Token ID: 307 -> Merged Bytes: b'ic' -> As String: 'ic'\n",
      "Token ID: 308 -> Merged Bytes: b'and' -> As String: 'and'\n",
      "Token ID: 309 -> Merged Bytes: b'ou' -> As String: 'ou'\n",
      "Token ID: 310 -> Merged Bytes: b'ec' -> As String: 'ec'\n",
      "Token ID: 311 -> Merged Bytes: b' a' -> As String: ' a'\n",
      "Token ID: 312 -> Merged Bytes: b').' -> As String: ').'\n",
      "Token ID: 313 -> Merged Bytes: b'rom' -> As String: 'rom'\n",
      "Token ID: 314 -> Merged Bytes: b' B' -> As String: ' B'\n",
      "Token ID: 315 -> Merged Bytes: b'ember' -> As String: 'ember'\n",
      "Token ID: 316 -> Merged Bytes: b' o' -> As String: ' o'\n",
      "Token ID: 317 -> Merged Bytes: b' from' -> As String: ' from'\n",
      "Token ID: 318 -> Merged Bytes: b' Ar' -> As String: ' Ar'\n",
      "Token ID: 319 -> Merged Bytes: b' and' -> As String: ' and'\n",
      "Token ID: 320 -> Merged Bytes: b' C' -> As String: ' C'\n",
      "Token ID: 321 -> Merged Bytes: b' N' -> As String: ' N'\n",
      "Token ID: 322 -> Merged Bytes: b' or' -> As String: ' or'\n",
      "Token ID: 323 -> Merged Bytes: b'chi' -> As String: 'chi'\n",
      "Token ID: 324 -> Merged Bytes: b' J' -> As String: ' J'\n",
      "Token ID: 325 -> Merged Bytes: b'inal' -> As String: 'inal'\n",
      "Token ID: 326 -> Merged Bytes: b' orig' -> As String: ' orig'\n",
      "Token ID: 327 -> Merged Bytes: b' original' -> As String: ' original'\n",
      "Token ID: 328 -> Merged Bytes: b' Archi' -> As String: ' Archi'\n",
      "Token ID: 329 -> Merged Bytes: b' Archived' -> As String: ' Archived'\n",
      "Token ID: 330 -> Merged Bytes: b' of' -> As String: ' of'\n",
      "Token ID: 331 -> Merged Bytes: b' h' -> As String: ' h'\n",
      "Token ID: 332 -> Merged Bytes: b' in' -> As String: ' in'\n",
      "Token ID: 333 -> Merged Bytes: b're' -> As String: 're'\n",
      "Token ID: 334 -> Merged Bytes: b'Taylor' -> As String: 'Taylor'\n",
      "Token ID: 335 -> Merged Bytes: b'it' -> As String: 'it'\n",
      "Token ID: 336 -> Merged Bytes: b'as' -> As String: 'as'\n",
      "Token ID: 337 -> Merged Bytes: b' p' -> As String: ' p'\n",
      "Token ID: 338 -> Merged Bytes: b'ion' -> As String: 'ion'\n",
      "Token ID: 339 -> Merged Bytes: b' D' -> As String: ' D'\n",
      "Token ID: 340 -> Merged Bytes: b' w' -> As String: ' w'\n",
      "Token ID: 341 -> Merged Bytes: b'ard' -> As String: 'ard'\n",
      "Token ID: 342 -> Merged Bytes: b'ill' -> As String: 'ill'\n",
      "Token ID: 343 -> Merged Bytes: b\"'s\" -> As String: ''s'\n",
      "Token ID: 344 -> Merged Bytes: b' m' -> As String: ' m'\n",
      "Token ID: 345 -> Merged Bytes: b' F' -> As String: ' F'\n",
      "Token ID: 346 -> Merged Bytes: b' W' -> As String: ' W'\n",
      "Token ID: 347 -> Merged Bytes: b'le' -> As String: 'le'\n",
      "Token ID: 348 -> Merged Bytes: b' to' -> As String: ' to'\n",
      "Token ID: 349 -> Merged Bytes: b' c' -> As String: ' c'\n",
      "Token ID: 350 -> Merged Bytes: b'.[' -> As String: '.['\n",
      "Token ID: 351 -> Merged Bytes: b'ov' -> As String: 'ov'\n",
      "Token ID: 352 -> Merged Bytes: b'ly' -> As String: 'ly'\n",
      "Token ID: 353 -> Merged Bytes: b'us' -> As String: 'us'\n",
      "Token ID: 354 -> Merged Bytes: b' H' -> As String: ' H'\n",
      "Token ID: 355 -> Merged Bytes: b'is' -> As String: 'is'\n",
      "Token ID: 356 -> Merged Bytes: b' P' -> As String: ' P'\n",
      "Token ID: 357 -> Merged Bytes: b'th' -> As String: 'th'\n",
      "Token ID: 358 -> Merged Bytes: b'ct' -> As String: 'ct'\n",
      "Token ID: 359 -> Merged Bytes: b'um' -> As String: 'um'\n",
      "Token ID: 360 -> Merged Bytes: b' b' -> As String: ' b'\n",
      "Token ID: 361 -> Merged Bytes: b' G' -> As String: ' G'\n",
      "Token ID: 362 -> Merged Bytes: b'ary' -> As String: 'ary'\n",
      "Token ID: 363 -> Merged Bytes: b' I' -> As String: ' I'\n",
      "Token ID: 364 -> Merged Bytes: b' Taylor' -> As String: ' Taylor'\n",
      "Token ID: 365 -> Merged Bytes: b'ist' -> As String: 'ist'\n",
      "Token ID: 366 -> Merged Bytes: b' d' -> As String: ' d'\n",
      "Token ID: 367 -> Merged Bytes: b'am' -> As String: 'am'\n",
      "Token ID: 368 -> Merged Bytes: b' O' -> As String: ' O'\n",
      "Token ID: 369 -> Merged Bytes: b'op' -> As String: 'op'\n",
      "Token ID: 370 -> Merged Bytes: b' Ju' -> As String: ' Ju'\n",
      "Token ID: 371 -> Merged Bytes: b' her' -> As String: ' her'\n",
      "Token ID: 372 -> Merged Bytes: b' L' -> As String: ' L'\n",
      "Token ID: 373 -> Merged Bytes: b'ust' -> As String: 'ust'\n",
      "Token ID: 374 -> Merged Bytes: b'ad' -> As String: 'ad'\n",
      "Token ID: 375 -> Merged Bytes: b' The' -> As String: ' The'\n",
      "Token ID: 376 -> Merged Bytes: b'uary' -> As String: 'uary'\n",
      "Token ID: 377 -> Merged Bytes: b'ent' -> As String: 'ent'\n",
      "Token ID: 378 -> Merged Bytes: b'usic' -> As String: 'usic'\n",
      "Token ID: 379 -> Merged Bytes: b'ovember' -> As String: 'ovember'\n",
      "Token ID: 380 -> Merged Bytes: b'ew' -> As String: 'ew'\n",
      "Token ID: 381 -> Merged Bytes: b'ers' -> As String: 'ers'\n",
      "Token ID: 382 -> Merged Bytes: b'art' -> As String: 'art'\n",
      "Token ID: 383 -> Merged Bytes: b'id' -> As String: 'id'\n",
      "Token ID: 384 -> Merged Bytes: b' E' -> As String: ' E'\n",
      "Token ID: 385 -> Merged Bytes: b'el' -> As String: 'el'\n",
      "Token ID: 386 -> Merged Bytes: b'ong' -> As String: 'ong'\n",
      "Token ID: 387 -> Merged Bytes: b'im' -> As String: 'im'\n",
      "Token ID: 388 -> Merged Bytes: b'ober' -> As String: 'ober'\n",
      "Token ID: 389 -> Merged Bytes: b'ep' -> As String: 'ep'\n",
      "Token ID: 390 -> Merged Bytes: b' for' -> As String: ' for'\n",
      "Token ID: 391 -> Merged Bytes: b'ctober' -> As String: 'ctober'\n",
      "Token ID: 392 -> Merged Bytes: b'bo' -> As String: 'bo'\n",
      "Token ID: 393 -> Merged Bytes: b' Bill' -> As String: ' Bill'\n",
      "Token ID: 394 -> Merged Bytes: b'ecember' -> As String: 'ecember'\n",
      "Token ID: 395 -> Merged Bytes: b' St' -> As String: ' St'\n",
      "Token ID: 396 -> Merged Bytes: b'board' -> As String: 'board'\n",
      "Token ID: 397 -> Merged Bytes: b'ow' -> As String: 'ow'\n",
      "Token ID: 398 -> Merged Bytes: b'ug' -> As String: 'ug'\n",
      "Token ID: 399 -> Merged Bytes: b'ot' -> As String: 'ot'\n",
      "Token ID: 400 -> Merged Bytes: b' Billboard' -> As String: ' Billboard'\n",
      "Token ID: 401 -> Merged Bytes: b'10' -> As String: '10'\n",
      "Token ID: 402 -> Merged Bytes: b'nt' -> As String: 'nt'\n",
      "Token ID: 403 -> Merged Bytes: b'ne' -> As String: 'ne'\n",
      "Token ID: 404 -> Merged Bytes: b'ear' -> As String: 'ear'\n",
      "Token ID: 405 -> Merged Bytes: b'our' -> As String: 'our'\n",
      "Token ID: 406 -> Merged Bytes: b'rit' -> As String: 'rit'\n",
      "Token ID: 407 -> Merged Bytes: b'ir' -> As String: 'ir'\n",
      "Token ID: 408 -> Merged Bytes: b'bum' -> As String: 'bum'\n",
      "Token ID: 409 -> Merged Bytes: b'ur' -> As String: 'ur'\n",
      "Token ID: 410 -> Merged Bytes: b'ation' -> As String: 'ation'\n",
      "Token ID: 411 -> Merged Bytes: b'200' -> As String: '200'\n",
      "Token ID: 412 -> Merged Bytes: b'ol' -> As String: 'ol'\n",
      "Token ID: 413 -> Merged Bytes: b'ess' -> As String: 'ess'\n",
      "Token ID: 414 -> Merged Bytes: b' r' -> As String: ' r'\n",
      "Token ID: 415 -> Merged Bytes: b'][' -> As String: ']['\n",
      "Token ID: 416 -> Merged Bytes: b'ugust' -> As String: 'ugust'\n",
      "Token ID: 417 -> Merged Bytes: b' V' -> As String: ' V'\n",
      "Token ID: 418 -> Merged Bytes: b\" '\" -> As String: ' ''\n",
      "Token ID: 419 -> Merged Bytes: b'eb' -> As String: 'eb'\n",
      "Token ID: 420 -> Merged Bytes: b' al' -> As String: ' al'\n",
      "Token ID: 421 -> Merged Bytes: b'iv' -> As String: 'iv'\n",
      "Token ID: 422 -> Merged Bytes: b' Y' -> As String: ' Y'\n",
      "Token ID: 423 -> Merged Bytes: b'ut' -> As String: 'ut'\n",
      "Token ID: 424 -> Merged Bytes: b' wi' -> As String: ' wi'\n",
      "Token ID: 425 -> Merged Bytes: b'ry' -> As String: 'ry'\n",
      "Token ID: 426 -> Merged Bytes: b'est' -> As String: 'est'\n",
      "Token ID: 427 -> Merged Bytes: b' with' -> As String: ' with'\n",
      "Token ID: 428 -> Merged Bytes: b'ra' -> As String: 'ra'\n",
      "Token ID: 429 -> Merged Bytes: b' December' -> As String: ' December'\n",
      "Token ID: 430 -> Merged Bytes: b' November' -> As String: ' November'\n",
      "Token ID: 431 -> Merged Bytes: b' at' -> As String: ' at'\n",
      "Token ID: 432 -> Merged Bytes: b' re' -> As String: ' re'\n",
      "Token ID: 433 -> Merged Bytes: b' July' -> As String: ' July'\n",
      "Token ID: 434 -> Merged Bytes: b' th' -> As String: ' th'\n",
      "Token ID: 435 -> Merged Bytes: b' n' -> As String: ' n'\n",
      "Token ID: 436 -> Merged Bytes: b' August' -> As String: ' August'\n",
      "Token ID: 437 -> Merged Bytes: b'ord' -> As String: 'ord'\n",
      "Token ID: 438 -> Merged Bytes: b'ward' -> As String: 'ward'\n",
      "Token ID: 439 -> Merged Bytes: b'19' -> As String: '19'\n",
      "Token ID: 440 -> Merged Bytes: b' K' -> As String: ' K'\n",
      "Token ID: 441 -> Merged Bytes: b' October' -> As String: ' October'\n",
      "Token ID: 442 -> Merged Bytes: b' l' -> As String: ' l'\n",
      "Token ID: 443 -> Merged Bytes: b'oll' -> As String: 'oll'\n",
      "Token ID: 444 -> Merged Bytes: b'pri' -> As String: 'pri'\n",
      "Token ID: 445 -> Merged Bytes: b'ul' -> As String: 'ul'\n",
      "Token ID: 446 -> Merged Bytes: b'ept' -> As String: 'ept'\n",
      "Token ID: 447 -> Merged Bytes: b' Music' -> As String: ' Music'\n",
      "Token ID: 448 -> Merged Bytes: b' U' -> As String: ' U'\n",
      "Token ID: 449 -> Merged Bytes: b'os' -> As String: 'os'\n",
      "Token ID: 450 -> Merged Bytes: b'ebr' -> As String: 'ebr'\n",
      "Token ID: 451 -> Merged Bytes: b' as' -> As String: ' as'\n",
      "Token ID: 452 -> Merged Bytes: b'nd' -> As String: 'nd'\n",
      "Token ID: 453 -> Merged Bytes: b',[' -> As String: ',['\n",
      "Token ID: 454 -> Merged Bytes: b'ebruary' -> As String: 'ebruary'\n",
      "Token ID: 455 -> Merged Bytes: b' New' -> As String: ' New'\n",
      "Token ID: 456 -> Merged Bytes: b' st' -> As String: ' st'\n",
      "Token ID: 457 -> Merged Bytes: b'od' -> As String: 'od'\n",
      "Token ID: 458 -> Merged Bytes: b'ak' -> As String: 'ak'\n",
      "Token ID: 459 -> Merged Bytes: b'pril' -> As String: 'pril'\n",
      "Token ID: 460 -> Merged Bytes: b'ount' -> As String: 'ount'\n",
      "Token ID: 461 -> Merged Bytes: b' Ch' -> As String: ' Ch'\n",
      "Token ID: 462 -> Merged Bytes: b'ce' -> As String: 'ce'\n",
      "Token ID: 463 -> Merged Bytes: b'eptember' -> As String: 'eptember'\n",
      "Token ID: 464 -> Merged Bytes: b' album' -> As String: ' album'\n",
      "Token ID: 465 -> Merged Bytes: b'il' -> As String: 'il'\n",
      "Token ID: 466 -> Merged Bytes: b' Award' -> As String: ' Award'\n",
      "Token ID: 467 -> Merged Bytes: b'ecord' -> As String: 'ecord'\n",
      "Token ID: 468 -> Merged Bytes: b'anuary' -> As String: 'anuary'\n",
      "Token ID: 469 -> Merged Bytes: b'13' -> As String: '13'\n",
      "Token ID: 470 -> Merged Bytes: b'ountry' -> As String: 'ountry'\n",
      "Token ID: 471 -> Merged Bytes: b'mer' -> As String: 'mer'\n",
      "Token ID: 472 -> Merged Bytes: b'one' -> As String: 'one'\n",
      "Token ID: 473 -> Merged Bytes: b' g' -> As String: ' g'\n",
      "Token ID: 474 -> Merged Bytes: b'21' -> As String: '21'\n",
      "Token ID: 475 -> Merged Bytes: b'arch' -> As String: 'arch'\n",
      "Token ID: 476 -> Merged Bytes: b'oc' -> As String: 'oc'\n",
      "Token ID: 477 -> Merged Bytes: b'ect' -> As String: 'ect'\n",
      "Token ID: 478 -> Merged Bytes: b'ia' -> As String: 'ia'\n",
      "Token ID: 479 -> Merged Bytes: b'ver' -> As String: 'ver'\n",
      "Token ID: 480 -> Merged Bytes: b'\\xe2\\x80' -> As String: 'ï¿½'\n",
      "Token ID: 481 -> Merged Bytes: b' She' -> As String: ' She'\n",
      "Token ID: 482 -> Merged Bytes: b' May' -> As String: ' May'\n",
      "Token ID: 483 -> Merged Bytes: b' Al' -> As String: ' Al'\n",
      "Token ID: 484 -> Merged Bytes: b'23' -> As String: '23'\n",
      "Token ID: 485 -> Merged Bytes: b' June' -> As String: ' June'\n",
      "Token ID: 486 -> Merged Bytes: b' music' -> As String: ' music'\n",
      "Token ID: 487 -> Merged Bytes: b'18' -> As String: '18'\n",
      "Token ID: 488 -> Merged Bytes: b' Tim' -> As String: ' Tim'\n",
      "Token ID: 489 -> Merged Bytes: b'ive' -> As String: 'ive'\n",
      "Token ID: 490 -> Merged Bytes: b' February' -> As String: ' February'\n",
      "Token ID: 491 -> Merged Bytes: b'ee' -> As String: 'ee'\n",
      "Token ID: 492 -> Merged Bytes: b'ian' -> As String: 'ian'\n",
      "Token ID: 493 -> Merged Bytes: b'24' -> As String: '24'\n",
      "Token ID: 494 -> Merged Bytes: b'meric' -> As String: 'meric'\n",
      "Token ID: 495 -> Merged Bytes: b'12' -> As String: '12'\n",
      "Token ID: 496 -> Merged Bytes: b' September' -> As String: ' September'\n",
      "Token ID: 497 -> Merged Bytes: b' sing' -> As String: ' sing'\n",
      "Token ID: 498 -> Merged Bytes: b' v' -> As String: ' v'\n",
      "Token ID: 499 -> Merged Bytes: b' was' -> As String: ' was'\n",
      "Token ID: 500 -> Merged Bytes: b' April' -> As String: ' April'\n",
      "Token ID: 501 -> Merged Bytes: b'17' -> As String: '17'\n",
      "Token ID: 502 -> Merged Bytes: b'22' -> As String: '22'\n",
      "Token ID: 503 -> Merged Bytes: b'og' -> As String: 'og'\n",
      "Token ID: 504 -> Merged Bytes: b'writ' -> As String: 'writ'\n",
      "Token ID: 505 -> Merged Bytes: b'November' -> As String: 'November'\n",
      "Token ID: 506 -> Merged Bytes: b' song' -> As String: ' song'\n",
      "Token ID: 507 -> Merged Bytes: b'14' -> As String: '14'\n",
      "Token ID: 508 -> Merged Bytes: b'igh' -> As String: 'igh'\n",
      "Token ID: 509 -> Merged Bytes: b'ld' -> As String: 'ld'\n",
      "Token ID: 510 -> Merged Bytes: b']\\n' -> As String: ']\n",
      "'\n",
      "Token ID: 511 -> Merged Bytes: b' she' -> As String: ' she'\n"
     ]
    }
   ],
   "source": [
    "# let's see our vocab\n",
    "for token, token_bytes in tokenizer2.vocab.items():\n",
    "    if token>255:\n",
    "        print(f\"Token ID: {token} -> Merged Bytes: {token_bytes} -> As String: '{token_bytes.decode('utf-8', 'replace')}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3ae76455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!!!? (ìëíì¸ì!) lol123 ð\n"
     ]
    }
   ],
   "source": [
    "# match this\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # this is the GPT-4 tokenizer\n",
    "ids = enc.encode(\"hello world!!!? (ìëíì¸ì!) lol123 ð\")\n",
    "text1 = enc.decode(ids) # get the same text back\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "507e17f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!!!? (ìëíì¸ì!) lol123 ð\n"
     ]
    }
   ],
   "source": [
    "ids2 = tokenizer2.encode(\"hello world!!!? (ìëíì¸ì!) lol123 ð\")\n",
    "text2 = tokenizer2.decode(ids2)\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c9b3125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"merges\": {f\"{p0},{p1}\": idx for (p0,p1), idx in tokenizer2.merges.items()},\n",
    "    \"vocab\": {str(idx): token_bytes.decode(\"latin-1\") for idx, token_bytes in tokenizer2.vocab.items()},\n",
    "    \"special_tokens\": tokenizer2.special_tokens\n",
    "}\n",
    "with open(\"regex_tokenizer.json\", \"w\") as f:\n",
    "    json.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6b113877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(mergeable_ranks, token, max_rank):\n",
    "    parts = [bytes([b]) for b in token]\n",
    "    while True:\n",
    "        min_idx = None\n",
    "        min_rank = None\n",
    "        for i , pair in enumerate(zip(parts[:-1], parts[1::])):\n",
    "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
    "            if rank is not None and (min_rank is None or rank < min_rank):\n",
    "                min_idx = i\n",
    "                min_rank = rank\n",
    "                \n",
    "        if min_rank is None or (max_rank is not None and min_rank >= max_rank):\n",
    "            break\n",
    "        assert min_idx is not None\n",
    "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx+1]] + parts[min_idx+2:]\n",
    "        \n",
    "    return parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "93964b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_merges(mergeable_ranks):\n",
    "    merges = {}\n",
    "    for token, rank in mergeable_ranks.items():\n",
    "        if len(token) ==1:\n",
    "            continue\n",
    "        pair = tuple(bpe(mergeable_ranks,token,rank))\n",
    "        assert len(pair)==2\n",
    "        rank1 = mergeable_ranks[pair[0]]\n",
    "        rank2 = mergeable_ranks[pair[1]]\n",
    "        merges[(rank1,rank2)]= rank\n",
    "    return merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "aa64fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "GPT4_SPECIAL_TOKENS = {\n",
    "    '<|endoftext|>': 100257,\n",
    "    '<|fim_prefix|>': 100258,\n",
    "    '<|fim_middle|>': 100259,\n",
    "    '<|fim_suffix|>': 100260,\n",
    "    '<|endofprompt|>': 100276\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "af1df562",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT4Tokenizer(RegexTokenizer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        mergeable_ranks = enc._mergeable_ranks\n",
    "        self.merges = recover_merges(mergeable_ranks)\n",
    "        # reconstruct the vocab from the merges\n",
    "        vocab = {idx:bytes([idx]) for idx in range(256)}\n",
    "        for (p0,p1),idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        self.vocab = vocab\n",
    "        self.byte_shuffle = {i:mergeable_ranks[bytes([i])] for i in range(256)}\n",
    "        self.inverse_byte_shuffle = {v:k for  k,v in self.byte_shuffle.items()}\n",
    "        self.register_special_tokens(GPT4_SPECIAL_TOKENS)\n",
    "        \n",
    "    def _encode_chunk(self,text_bytes):\n",
    "        text_bytes = bytes(self.byte_shuffle[b] for b in text_bytes)\n",
    "        ids = super()._encode_chunk(text_bytes)\n",
    "        return ids\n",
    "    \n",
    "    def decode(self,ids):\n",
    "        text_bytes = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text_bytes = bytes(self.inverse_byte_shuffle[b] for b in text_bytes)\n",
    "        text = text_bytes.decode(\"utf-8\",errors=\"replace\")\n",
    "        return text\n",
    "    \n",
    "    def train(self,text,vocab_size,verbose=False):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def save(self,file_prefix):\n",
    "        raise NotImplementedError(\"GPT4Tokenizer cannot be saved.\")\n",
    "\n",
    "    def load(self, model_file):\n",
    "        raise NotImplementedError(\"GPT4Tokenizer cannot be loaded.\")\n",
    "        \n",
    "    def save_vocab(self,vocab_file):\n",
    "        vocab = {idx:bytes([self.inverse_byte_shuffle[idx]]) for idx in range(256)}\n",
    "        for (p0,p1),idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        #  # now merge the shuffled bytes and write to file\n",
    "        inverted_merges = {idx: pair for pair, idx in self.merges.items()}\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for idx, token in vocab.items():\n",
    "                s = render_token(token)\n",
    "                if idx in inverted_merges:\n",
    "                    idx0, idx1 = inverted_merges[idx]\n",
    "                    s0 = render_token(vocab[idx0])\n",
    "                    s1 = render_token(vocab[idx1])\n",
    "                    f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"[{s}] {idx}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7bd3badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer3 = GPT4Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a4f3c815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!!!? (ìëíì¸ì!) lol123 ð\n"
     ]
    }
   ],
   "source": [
    "ids3 = tokenizer3.encode(\"hello world!!!? (ìëíì¸ì!) lol123 ð\")\n",
    "text3 = tokenizer3.decode(ids3)\n",
    "print(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2c3b18ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 9809, 262, 4435]\n"
     ]
    }
   ],
   "source": [
    "ids3 = tokenizer3.encode(\"Hello/n    World\")\n",
    "text3 = tokenizer3.decode(ids3)\n",
    "print(ids3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c687fc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 301, 111, 47, 110, 32, 32, 32, 346, 258, 509]\n",
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids2 = tokenizer2.encode(\"Hello/n    World\")\n",
    "text2 = tokenizer2.decode(ids2)\n",
    "print(ids2), print(len(ids2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b781d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661e295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
